00:21:37,622 root INFO ASRModel(
  (encoder): TransformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=5120, out_features=256, bias=True)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (ctc): CTC(
    (projection): Linear(in_features=256, out_features=302, bias=True)
    (ctc_loss): CTCLoss()
  )
  (stat_calculator): StatsCalculator()
)
00:21:37,623 root INFO Built a model with 11.46M Params
00:21:37,629 root INFO Start to train epoch 0
00:23:06,839 root INFO [Epoch 0, Batch=99] Train: loss=620.9035, wer=1.0000, lr=3.3333333333333337e-06
00:24:18,884 root INFO [Epoch 0, Batch=199] Train: loss=262.5892, wer=1.0000, lr=6.6666666666666675e-06
00:25:25,889 root INFO [Epoch 0, Batch=299] Train: loss=177.9193, wer=1.0000, lr=1e-05
00:26:33,845 root INFO [Epoch 0, Batch=399] Train: loss=79.5441, wer=1.0000, lr=1.3333333333333335e-05
00:27:41,511 root INFO [Epoch 0, Batch=499] Train: loss=241.8519, wer=1.0000, lr=1.666666666666667e-05
00:28:48,902 root INFO [Epoch 0, Batch=599] Train: loss=247.9184, wer=1.0000, lr=2e-05
00:29:56,429 root INFO [Epoch 0, Batch=699] Train: loss=226.9916, wer=1.0000, lr=2.3333333333333336e-05
00:31:04,81 root INFO [Epoch 0, Batch=799] Train: loss=158.7202, wer=1.0000, lr=2.666666666666667e-05
00:32:11,685 root INFO [Epoch 0, Batch=899] Train: loss=242.2002, wer=1.0000, lr=3.0000000000000004e-05
00:33:19,184 root INFO [Epoch 0, Batch=999] Train: loss=240.8141, wer=1.0000, lr=3.333333333333334e-05
00:34:26,951 root INFO [Epoch 0, Batch=1099] Train: loss=232.2061, wer=1.0000, lr=3.666666666666667e-05
00:35:35,72 root INFO [Epoch 0, Batch=1199] Train: loss=221.7906, wer=0.9877, lr=4e-05
00:36:44,357 root INFO [Epoch 0, Batch=1299] Train: loss=226.6144, wer=0.9774, lr=4.333333333333334e-05
00:37:55,727 root INFO [Epoch 0, Batch=1399] Train: loss=194.5802, wer=0.9733, lr=4.666666666666667e-05
00:39:07,986 root INFO [Epoch 0, Batch=1499] Train: loss=64.9048, wer=0.9802, lr=5e-05
00:40:20,777 root INFO [Epoch 0, Batch=1599] Train: loss=213.2709, wer=0.9760, lr=5.333333333333334e-05
00:41:34,200 root INFO [Epoch 0, Batch=1699] Train: loss=192.9584, wer=0.9638, lr=5.666666666666667e-05
00:42:47,622 root INFO [Epoch 0, Batch=1799] Train: loss=216.3648, wer=0.9544, lr=6.000000000000001e-05
00:44:01,792 root INFO [Epoch 0, Batch=1899] Train: loss=216.7868, wer=0.9618, lr=6.333333333333335e-05
00:45:08,789 root INFO Start to validate epoch 0
00:45:30,128 root INFO Epoch 00, lr=6.64e-05 | Train: loss=250.7257, wer=1.0278 | Val: loss=273.2503, wer=0.9308 | Time: this epoch 1432.50s, elapsed 1432.50s
00:45:30,358 root INFO [info] Save model after epoch 0

00:45:30,359 root INFO Start to train epoch 1
00:46:44,800 root INFO [Epoch 1, Batch=99] Train: loss=218.4003, wer=0.9386, lr=6.966666666666668e-05
00:47:58,549 root INFO [Epoch 1, Batch=199] Train: loss=195.8578, wer=0.9332, lr=7.300000000000001e-05
00:49:13,120 root INFO [Epoch 1, Batch=299] Train: loss=170.8432, wer=0.9382, lr=7.633333333333335e-05
00:50:29,38 root INFO [Epoch 1, Batch=399] Train: loss=208.8440, wer=0.9474, lr=7.966666666666669e-05
00:51:44,477 root INFO [Epoch 1, Batch=499] Train: loss=188.1391, wer=0.9481, lr=8.300000000000001e-05
00:53:00,95 root INFO [Epoch 1, Batch=599] Train: loss=183.3594, wer=0.9336, lr=8.633333333333335e-05
00:54:16,233 root INFO [Epoch 1, Batch=699] Train: loss=116.3231, wer=0.9395, lr=8.966666666666669e-05
00:55:32,459 root INFO [Epoch 1, Batch=799] Train: loss=175.5495, wer=0.9393, lr=9.300000000000001e-05
00:56:48,795 root INFO [Epoch 1, Batch=899] Train: loss=176.7559, wer=0.9411, lr=9.633333333333335e-05
00:58:05,143 root INFO [Epoch 1, Batch=999] Train: loss=172.1477, wer=0.9215, lr=9.966666666666669e-05
00:59:20,50 root INFO [Epoch 1, Batch=1099] Train: loss=189.2629, wer=0.9205, lr=0.00010300000000000001
01:00:36,82 root INFO [Epoch 1, Batch=1199] Train: loss=181.1493, wer=0.9261, lr=0.00010633333333333335
01:01:51,668 root INFO [Epoch 1, Batch=1299] Train: loss=164.2935, wer=0.9168, lr=0.00010966666666666669
01:03:07,329 root INFO [Epoch 1, Batch=1399] Train: loss=194.5979, wer=0.9334, lr=0.00011300000000000001
01:04:22,858 root INFO [Epoch 1, Batch=1499] Train: loss=184.7857, wer=0.9194, lr=0.00011633333333333335
01:05:38,630 root INFO [Epoch 1, Batch=1599] Train: loss=181.1507, wer=0.9140, lr=0.0001196666666666667
01:06:54,245 root INFO [Epoch 1, Batch=1699] Train: loss=171.5167, wer=0.9231, lr=0.00012300000000000004
01:08:11,491 root INFO [Epoch 1, Batch=1799] Train: loss=189.6324, wer=0.9089, lr=0.00012633333333333336
01:09:28,132 root INFO [Epoch 1, Batch=1899] Train: loss=154.9187, wer=0.9187, lr=0.00012966666666666668
01:10:37,252 root INFO Start to validate epoch 1
01:10:59,534 root INFO Epoch 01, lr=0.00013273333333333335 | Train: loss=173.3773, wer=0.9326 | Val: loss=215.0408, wer=0.8943 | Time: this epoch 1529.18s, elapsed 2961.91s
01:10:59,805 root INFO [info] Save model after epoch 1

01:10:59,805 root INFO Start to train epoch 2
01:12:17,527 root INFO [Epoch 2, Batch=99] Train: loss=167.7254, wer=0.9072, lr=0.000136
01:13:34,6 root INFO [Epoch 2, Batch=199] Train: loss=194.8252, wer=0.9054, lr=0.00013933333333333335
01:14:50,71 root INFO [Epoch 2, Batch=299] Train: loss=171.8816, wer=0.9181, lr=0.0001426666666666667
01:16:07,1 root INFO [Epoch 2, Batch=399] Train: loss=165.1514, wer=0.9070, lr=0.00014600000000000003
01:17:23,760 root INFO [Epoch 2, Batch=499] Train: loss=178.0155, wer=0.9055, lr=0.00014933333333333338
01:18:40,687 root INFO [Epoch 2, Batch=599] Train: loss=168.2235, wer=0.9130, lr=0.0001526666666666667
01:19:57,70 root INFO [Epoch 2, Batch=699] Train: loss=175.2475, wer=0.9259, lr=0.00015600000000000002
01:21:13,958 root INFO [Epoch 2, Batch=799] Train: loss=151.0838, wer=0.8905, lr=0.00015933333333333338
01:22:30,995 root INFO [Epoch 2, Batch=899] Train: loss=155.8590, wer=0.8882, lr=0.0001626666666666667
01:23:47,954 root INFO [Epoch 2, Batch=999] Train: loss=121.9136, wer=0.9149, lr=0.00016600000000000002
01:25:04,541 root INFO [Epoch 2, Batch=1099] Train: loss=127.1793, wer=0.9058, lr=0.00016933333333333337
01:26:21,281 root INFO [Epoch 2, Batch=1199] Train: loss=97.8290, wer=0.8939, lr=0.0001726666666666667
01:27:38,124 root INFO [Epoch 2, Batch=1299] Train: loss=149.6599, wer=0.8992, lr=0.00017600000000000002
01:28:54,869 root INFO [Epoch 2, Batch=1399] Train: loss=170.3406, wer=0.8941, lr=0.00017933333333333337
01:30:12,153 root INFO [Epoch 2, Batch=1499] Train: loss=143.9567, wer=0.8898, lr=0.0001826666666666667
01:31:29,478 root INFO [Epoch 2, Batch=1599] Train: loss=146.3236, wer=0.9021, lr=0.00018600000000000002
01:32:46,848 root INFO [Epoch 2, Batch=1699] Train: loss=168.2053, wer=0.9028, lr=0.00018933333333333337
01:34:04,453 root INFO [Epoch 2, Batch=1799] Train: loss=154.6463, wer=0.8759, lr=0.0001926666666666667
01:35:21,468 root INFO [Epoch 2, Batch=1899] Train: loss=166.4804, wer=0.8981, lr=0.00019600000000000002
01:36:31,11 root INFO Start to validate epoch 2
01:36:53,958 root INFO Epoch 02, lr=0.0001990666666666667 | Train: loss=152.7761, wer=0.9010 | Val: loss=189.7335, wer=0.8606 | Time: this epoch 1554.15s, elapsed 4516.33s
01:36:54,232 root INFO [info] Save model after epoch 2

01:36:54,232 root INFO Start to train epoch 3
01:38:12,235 root INFO [Epoch 3, Batch=99] Train: loss=132.1175, wer=0.8995, lr=0.00020233333333333336
01:39:29,770 root INFO [Epoch 3, Batch=199] Train: loss=146.0820, wer=0.8742, lr=0.00020566666666666671
01:40:47,91 root INFO [Epoch 3, Batch=299] Train: loss=136.9113, wer=0.8655, lr=0.00020900000000000004
01:42:04,929 root INFO [Epoch 3, Batch=399] Train: loss=26.2790, wer=0.9015, lr=0.00021233333333333336
01:43:22,771 root INFO [Epoch 3, Batch=499] Train: loss=149.4915, wer=0.8743, lr=0.00021566666666666671
01:44:40,339 root INFO [Epoch 3, Batch=599] Train: loss=124.1326, wer=0.8742, lr=0.00021900000000000004
01:45:58,236 root INFO [Epoch 3, Batch=699] Train: loss=156.2343, wer=0.8533, lr=0.00022233333333333336
01:47:15,812 root INFO [Epoch 3, Batch=799] Train: loss=142.6737, wer=0.8973, lr=0.0002256666666666667
01:48:33,363 root INFO [Epoch 3, Batch=899] Train: loss=164.0652, wer=0.8755, lr=0.00022900000000000004
01:49:51,54 root INFO [Epoch 3, Batch=999] Train: loss=138.1329, wer=0.8877, lr=0.00023233333333333336
01:51:09,343 root INFO [Epoch 3, Batch=1099] Train: loss=147.3763, wer=0.8474, lr=0.0002356666666666667
01:52:27,160 root INFO [Epoch 3, Batch=1199] Train: loss=164.1453, wer=0.8775, lr=0.00023900000000000004
01:53:45,260 root INFO [Epoch 3, Batch=1299] Train: loss=148.8351, wer=0.8367, lr=0.00024233333333333336
01:55:03,85 root INFO [Epoch 3, Batch=1399] Train: loss=139.6154, wer=0.8515, lr=0.0002456666666666667
01:56:21,677 root INFO [Epoch 3, Batch=1499] Train: loss=116.1398, wer=0.8620, lr=0.00024900000000000004
01:57:39,769 root INFO [Epoch 3, Batch=1599] Train: loss=145.0294, wer=0.8658, lr=0.00025233333333333336
01:58:58,10 root INFO [Epoch 3, Batch=1699] Train: loss=68.9812, wer=0.8579, lr=0.0002556666666666667
02:00:16,241 root INFO [Epoch 3, Batch=1799] Train: loss=129.4537, wer=0.8454, lr=0.00025900000000000006
02:01:34,605 root INFO [Epoch 3, Batch=1899] Train: loss=147.7854, wer=0.8698, lr=0.0002623333333333334
02:02:44,907 root INFO Start to validate epoch 3
02:03:08,821 root INFO Epoch 03, lr=0.00026540000000000005 | Train: loss=141.0698, wer=0.8738 | Val: loss=175.3639, wer=0.8304 | Time: this epoch 1574.59s, elapsed 6091.19s
02:03:09,87 root INFO [info] Save model after epoch 3

02:03:09,87 root INFO Start to train epoch 4
02:04:28,121 root INFO [Epoch 4, Batch=99] Train: loss=139.9734, wer=0.8538, lr=0.0002686666666666667
02:05:46,509 root INFO [Epoch 4, Batch=199] Train: loss=137.4423, wer=0.8510, lr=0.000272
02:07:04,955 root INFO [Epoch 4, Batch=299] Train: loss=62.9205, wer=0.8449, lr=0.0002753333333333333
02:08:23,676 root INFO [Epoch 4, Batch=399] Train: loss=141.6036, wer=0.8568, lr=0.0002786666666666667
02:09:42,590 root INFO [Epoch 4, Batch=499] Train: loss=125.6921, wer=0.8730, lr=0.0002820000000000001
02:11:01,106 root INFO [Epoch 4, Batch=599] Train: loss=139.1860, wer=0.8469, lr=0.0002853333333333334
02:12:19,966 root INFO [Epoch 4, Batch=699] Train: loss=115.0660, wer=0.8585, lr=0.0002886666666666667
02:13:38,530 root INFO [Epoch 4, Batch=799] Train: loss=156.2401, wer=0.8692, lr=0.00029200000000000005
02:14:57,704 root INFO [Epoch 4, Batch=899] Train: loss=108.2094, wer=0.8453, lr=0.0002953333333333334
02:16:16,605 root INFO [Epoch 4, Batch=999] Train: loss=115.8551, wer=0.8427, lr=0.00029866666666666675
02:17:35,606 root INFO [Epoch 4, Batch=1099] Train: loss=107.1017, wer=0.8358, lr=0.0003020000000000001
02:18:54,651 root INFO [Epoch 4, Batch=1199] Train: loss=129.6670, wer=0.8292, lr=0.0003053333333333334
02:20:13,801 root INFO [Epoch 4, Batch=1299] Train: loss=144.0226, wer=0.8420, lr=0.0003086666666666667
02:21:33,695 root INFO [Epoch 4, Batch=1399] Train: loss=142.2175, wer=0.8487, lr=0.00031200000000000005
02:22:52,541 root INFO [Epoch 4, Batch=1499] Train: loss=120.9310, wer=0.8336, lr=0.00031533333333333337
02:24:12,307 root INFO [Epoch 4, Batch=1599] Train: loss=128.9603, wer=0.8264, lr=0.00031866666666666675
02:25:32,112 root INFO [Epoch 4, Batch=1699] Train: loss=105.9750, wer=0.8241, lr=0.0003220000000000001
02:26:52,474 root INFO [Epoch 4, Batch=1799] Train: loss=105.4916, wer=0.8056, lr=0.0003253333333333334
02:28:12,658 root INFO [Epoch 4, Batch=1899] Train: loss=137.4037, wer=0.8085, lr=0.0003286666666666667
02:29:24,578 root INFO Start to validate epoch 4
02:29:48,277 root INFO Epoch 04, lr=0.0003317333333333334 | Train: loss=129.9202, wer=0.8415 | Val: loss=145.3730, wer=0.7515 | Time: this epoch 1599.19s, elapsed 7690.65s
02:29:48,549 root INFO [info] Save model after epoch 4

02:29:48,549 root INFO Start to train epoch 5
02:31:10,55 root INFO [Epoch 5, Batch=99] Train: loss=130.8945, wer=0.8173, lr=0.00033500000000000007
02:32:30,389 root INFO [Epoch 5, Batch=199] Train: loss=126.0525, wer=0.8078, lr=0.0003383333333333334
02:33:51,22 root INFO [Epoch 5, Batch=299] Train: loss=104.9357, wer=0.7914, lr=0.0003416666666666667
02:35:12,382 root INFO [Epoch 5, Batch=399] Train: loss=119.2901, wer=0.7920, lr=0.00034500000000000004
02:36:33,607 root INFO [Epoch 5, Batch=499] Train: loss=135.6273, wer=0.8093, lr=0.00034833333333333336
02:37:55,330 root INFO [Epoch 5, Batch=599] Train: loss=110.2911, wer=0.7745, lr=0.0003516666666666667
02:39:16,963 root INFO [Epoch 5, Batch=699] Train: loss=121.0897, wer=0.7687, lr=0.00035500000000000006
02:40:39,290 root INFO [Epoch 5, Batch=799] Train: loss=113.8827, wer=0.8179, lr=0.0003583333333333334
02:42:00,984 root INFO [Epoch 5, Batch=899] Train: loss=110.3581, wer=0.7697, lr=0.0003616666666666667
02:43:22,808 root INFO [Epoch 5, Batch=999] Train: loss=118.6417, wer=0.7770, lr=0.00036500000000000004
02:44:45,512 root INFO [Epoch 5, Batch=1099] Train: loss=128.6723, wer=0.7726, lr=0.00036833333333333336
02:46:07,904 root INFO [Epoch 5, Batch=1199] Train: loss=114.4372, wer=0.7430, lr=0.0003716666666666667
02:47:30,97 root INFO [Epoch 5, Batch=1299] Train: loss=94.6903, wer=0.7330, lr=0.00037500000000000006
02:48:52,941 root INFO [Epoch 5, Batch=1399] Train: loss=108.9579, wer=0.7364, lr=0.0003783333333333334
02:50:15,549 root INFO [Epoch 5, Batch=1499] Train: loss=58.0763, wer=0.7518, lr=0.0003816666666666667
02:51:38,424 root INFO [Epoch 5, Batch=1599] Train: loss=111.9525, wer=0.7294, lr=0.00038500000000000003
02:53:01,835 root INFO [Epoch 5, Batch=1699] Train: loss=121.7147, wer=0.7404, lr=0.00038833333333333336
02:54:24,830 root INFO [Epoch 5, Batch=1799] Train: loss=98.2626, wer=0.7220, lr=0.0003916666666666667
02:55:48,154 root INFO [Epoch 5, Batch=1899] Train: loss=91.2228, wer=0.7133, lr=0.00039500000000000006
02:57:03,116 root INFO Start to validate epoch 5
02:57:28,185 root INFO Epoch 05, lr=0.0003980666666666667 | Train: loss=107.2651, wer=0.7610 | Val: loss=101.7573, wer=0.6066 | Time: this epoch 1659.64s, elapsed 9350.56s
02:57:28,450 root INFO [info] Save model after epoch 5

02:57:28,451 root INFO Start to train epoch 6
02:58:52,671 root INFO [Epoch 6, Batch=99] Train: loss=100.3634, wer=0.7178, lr=0.0004013333333333334
03:00:16,384 root INFO [Epoch 6, Batch=199] Train: loss=74.3076, wer=0.7312, lr=0.0004046666666666667
03:01:39,712 root INFO [Epoch 6, Batch=299] Train: loss=87.0791, wer=0.6412, lr=0.00040800000000000005
03:03:03,875 root INFO [Epoch 6, Batch=399] Train: loss=93.8485, wer=0.7080, lr=0.00041133333333333343
03:04:27,804 root INFO [Epoch 6, Batch=499] Train: loss=107.3382, wer=0.7006, lr=0.00041466666666666675
03:05:51,827 root INFO [Epoch 6, Batch=599] Train: loss=79.3336, wer=0.6549, lr=0.0004180000000000001
03:07:16,0 root INFO [Epoch 6, Batch=699] Train: loss=96.2015, wer=0.6520, lr=0.0004213333333333334
03:08:40,375 root INFO [Epoch 6, Batch=799] Train: loss=131.9504, wer=0.6725, lr=0.0004246666666666667
03:10:05,29 root INFO [Epoch 6, Batch=899] Train: loss=94.9533, wer=0.6758, lr=0.00042800000000000005
03:11:28,863 root INFO [Epoch 6, Batch=999] Train: loss=85.5017, wer=0.6553, lr=0.00043133333333333343
03:12:53,95 root INFO [Epoch 6, Batch=1099] Train: loss=82.3063, wer=0.6372, lr=0.00043466666666666675
03:14:17,978 root INFO [Epoch 6, Batch=1199] Train: loss=94.4566, wer=0.6456, lr=0.0004380000000000001
03:15:42,641 root INFO [Epoch 6, Batch=1299] Train: loss=92.9414, wer=0.6434, lr=0.0004413333333333334
03:17:07,779 root INFO [Epoch 6, Batch=1399] Train: loss=67.2095, wer=0.7276, lr=0.0004446666666666667
03:18:32,550 root INFO [Epoch 6, Batch=1499] Train: loss=84.0745, wer=0.6184, lr=0.00044800000000000005
03:19:57,456 root INFO [Epoch 6, Batch=1599] Train: loss=103.3125, wer=0.6448, lr=0.0004513333333333334
03:21:22,208 root INFO [Epoch 6, Batch=1699] Train: loss=84.4034, wer=0.6148, lr=0.00045466666666666675
03:22:46,828 root INFO [Epoch 6, Batch=1799] Train: loss=15.8358, wer=0.7453, lr=0.0004580000000000001
03:24:12,284 root INFO [Epoch 6, Batch=1899] Train: loss=76.6835, wer=0.6544, lr=0.0004613333333333334
03:25:28,779 root INFO Start to validate epoch 6
03:25:54,328 root INFO Epoch 06, lr=0.00046440000000000007 | Train: loss=86.7984, wer=0.6634 | Val: loss=79.1792, wer=0.4958 | Time: this epoch 1705.88s, elapsed 11056.70s
03:25:54,595 root INFO [info] Save model after epoch 6

03:25:54,595 root INFO Start to train epoch 7
03:27:20,601 root INFO [Epoch 7, Batch=99] Train: loss=99.5446, wer=0.6438, lr=0.00046766666666666674
03:28:45,546 root INFO [Epoch 7, Batch=199] Train: loss=82.6673, wer=0.5896, lr=0.00047100000000000006
03:30:10,665 root INFO [Epoch 7, Batch=299] Train: loss=61.5254, wer=0.6257, lr=0.0004743333333333334
03:31:35,661 root INFO [Epoch 7, Batch=399] Train: loss=96.5520, wer=0.6415, lr=0.0004776666666666667
03:33:01,223 root INFO [Epoch 7, Batch=499] Train: loss=99.3945, wer=0.6209, lr=0.00048100000000000004
03:34:26,469 root INFO [Epoch 7, Batch=599] Train: loss=43.4087, wer=0.5961, lr=0.00048433333333333336
03:35:52,260 root INFO [Epoch 7, Batch=699] Train: loss=68.4459, wer=0.5796, lr=0.00048766666666666674
03:37:17,278 root INFO [Epoch 7, Batch=799] Train: loss=78.1506, wer=0.5544, lr=0.000491
03:38:43,83 root INFO [Epoch 7, Batch=899] Train: loss=90.1734, wer=0.6074, lr=0.0004943333333333334
03:40:08,510 root INFO [Epoch 7, Batch=999] Train: loss=83.8217, wer=0.5848, lr=0.0004976666666666668
03:41:34,82 root INFO [Epoch 7, Batch=1099] Train: loss=79.6673, wer=0.5585, lr=0.000501
03:43:00,305 root INFO [Epoch 7, Batch=1199] Train: loss=106.4602, wer=0.6331, lr=0.0005043333333333334
03:44:25,636 root INFO [Epoch 7, Batch=1299] Train: loss=80.4342, wer=0.5580, lr=0.0005076666666666667
03:45:51,233 root INFO [Epoch 7, Batch=1399] Train: loss=76.1994, wer=0.5614, lr=0.0005110000000000001
03:47:17,101 root INFO [Epoch 7, Batch=1499] Train: loss=85.7035, wer=0.5764, lr=0.0005143333333333333
03:48:43,330 root INFO [Epoch 7, Batch=1599] Train: loss=74.7756, wer=0.5584, lr=0.0005176666666666667
03:50:09,7 root INFO [Epoch 7, Batch=1699] Train: loss=65.5916, wer=0.5474, lr=0.0005210000000000001
03:51:35,97 root INFO [Epoch 7, Batch=1799] Train: loss=74.8110, wer=0.5576, lr=0.0005243333333333334
03:53:00,840 root INFO [Epoch 7, Batch=1899] Train: loss=39.5793, wer=0.5940, lr=0.0005276666666666667
03:54:18,30 root INFO Start to validate epoch 7
03:54:44,117 root INFO Epoch 07, lr=0.0005307333333333334 | Train: loss=74.9225, wer=0.5908 | Val: loss=64.9719, wer=0.4205 | Time: this epoch 1729.52s, elapsed 12786.49s
03:54:44,376 root INFO [info] Save model after epoch 7

03:54:44,376 root INFO Start to train epoch 8
03:56:11,44 root INFO [Epoch 8, Batch=99] Train: loss=56.8899, wer=0.5558, lr=0.0005340000000000001
03:57:36,856 root INFO [Epoch 8, Batch=199] Train: loss=80.5054, wer=0.5691, lr=0.0005373333333333333
03:59:03,467 root INFO [Epoch 8, Batch=299] Train: loss=57.5499, wer=0.5123, lr=0.0005406666666666667
04:00:29,264 root INFO [Epoch 8, Batch=399] Train: loss=49.1545, wer=0.5175, lr=0.000544
04:01:55,318 root INFO [Epoch 8, Batch=499] Train: loss=38.0258, wer=0.5279, lr=0.0005473333333333334
04:03:21,286 root INFO [Epoch 8, Batch=599] Train: loss=76.8648, wer=0.5262, lr=0.0005506666666666666
04:04:47,588 root INFO [Epoch 8, Batch=699] Train: loss=69.5004, wer=0.5083, lr=0.000554
04:06:13,246 root INFO [Epoch 8, Batch=799] Train: loss=87.1842, wer=0.5333, lr=0.0005573333333333334
04:07:39,976 root INFO [Epoch 8, Batch=899] Train: loss=64.9232, wer=0.5163, lr=0.0005606666666666668
04:09:05,873 root INFO [Epoch 8, Batch=999] Train: loss=57.7899, wer=0.5334, lr=0.0005640000000000002
04:10:32,110 root INFO [Epoch 8, Batch=1099] Train: loss=61.8123, wer=0.5319, lr=0.0005673333333333334
04:11:57,998 root INFO [Epoch 8, Batch=1199] Train: loss=77.4173, wer=0.5718, lr=0.0005706666666666668
04:13:24,527 root INFO [Epoch 8, Batch=1299] Train: loss=71.1408, wer=0.5371, lr=0.0005740000000000002
04:14:50,336 root INFO [Epoch 8, Batch=1399] Train: loss=58.8390, wer=0.5653, lr=0.0005773333333333335
04:16:16,662 root INFO [Epoch 8, Batch=1499] Train: loss=65.0493, wer=0.4909, lr=0.0005806666666666668
04:17:42,847 root INFO [Epoch 8, Batch=1599] Train: loss=76.3581, wer=0.5238, lr=0.0005840000000000001
04:19:08,610 root INFO [Epoch 8, Batch=1699] Train: loss=75.8959, wer=0.5126, lr=0.0005873333333333335
04:20:35,300 root INFO [Epoch 8, Batch=1799] Train: loss=74.6788, wer=0.5370, lr=0.0005906666666666667
04:22:01,63 root INFO [Epoch 8, Batch=1899] Train: loss=57.6046, wer=0.5463, lr=0.0005940000000000001
04:23:19,106 root INFO Start to validate epoch 8
04:23:44,609 root INFO Epoch 08, lr=0.0005970666666666667 | Train: loss=67.4431, wer=0.5411 | Val: loss=58.5096, wer=0.3869 | Time: this epoch 1740.23s, elapsed 14526.98s
04:23:44,876 root INFO [info] Save model after epoch 8

04:23:44,876 root INFO Start to train epoch 9
04:25:11,955 root INFO [Epoch 9, Batch=99] Train: loss=67.5191, wer=0.4972, lr=0.0006003333333333335
04:26:38,153 root INFO [Epoch 9, Batch=199] Train: loss=71.8686, wer=0.5215, lr=0.0006036666666666667
04:28:04,812 root INFO [Epoch 9, Batch=299] Train: loss=57.6747, wer=0.5269, lr=0.0006070000000000001
04:29:30,976 root INFO [Epoch 9, Batch=399] Train: loss=62.6457, wer=0.5843, lr=0.0006103333333333335
04:30:57,359 root INFO [Epoch 9, Batch=499] Train: loss=80.8462, wer=0.5477, lr=0.0006136666666666668
04:32:23,554 root INFO [Epoch 9, Batch=599] Train: loss=55.1568, wer=0.4967, lr=0.0006170000000000001
04:33:50,14 root INFO [Epoch 9, Batch=699] Train: loss=29.4253, wer=0.5851, lr=0.0006203333333333334
04:35:15,823 root INFO [Epoch 9, Batch=799] Train: loss=61.3271, wer=0.4323, lr=0.0006236666666666668
04:36:42,428 root INFO [Epoch 9, Batch=899] Train: loss=73.3249, wer=0.5513, lr=0.0006270000000000001
04:38:08,785 root INFO [Epoch 9, Batch=999] Train: loss=60.8695, wer=0.4925, lr=0.0006303333333333334
04:39:35,205 root INFO [Epoch 9, Batch=1099] Train: loss=48.7280, wer=0.4796, lr=0.0006336666666666668
04:41:01,443 root INFO [Epoch 9, Batch=1199] Train: loss=63.8044, wer=0.4864, lr=0.0006370000000000001
04:42:28,564 root INFO [Epoch 9, Batch=1299] Train: loss=58.6800, wer=0.5191, lr=0.0006403333333333335
04:43:54,917 root INFO [Epoch 9, Batch=1399] Train: loss=76.8433, wer=0.5383, lr=0.0006436666666666667
04:45:21,640 root INFO [Epoch 9, Batch=1499] Train: loss=63.8324, wer=0.4742, lr=0.0006470000000000001
04:46:47,831 root INFO [Epoch 9, Batch=1599] Train: loss=58.8753, wer=0.4677, lr=0.0006503333333333335
04:48:14,324 root INFO [Epoch 9, Batch=1699] Train: loss=56.1359, wer=0.5193, lr=0.0006536666666666668
04:49:40,600 root INFO [Epoch 9, Batch=1799] Train: loss=59.3981, wer=0.5006, lr=0.0006570000000000001
04:51:07,225 root INFO [Epoch 9, Batch=1899] Train: loss=64.5803, wer=0.4734, lr=0.0006603333333333334
04:52:24,669 root INFO Start to validate epoch 9
04:52:50,536 root INFO Epoch 09, lr=0.0006634000000000001 | Train: loss=63.0079, wer=0.5086 | Val: loss=53.9079, wer=0.3510 | Time: this epoch 1745.66s, elapsed 16272.91s
04:52:50,803 root INFO [info] Save model after epoch 9

04:52:50,804 root INFO Start to train epoch 10
04:54:18,107 root INFO [Epoch 10, Batch=99] Train: loss=54.3811, wer=0.4508, lr=0.0006666666666666668
04:55:44,515 root INFO [Epoch 10, Batch=199] Train: loss=49.9655, wer=0.4734, lr=0.0006700000000000001
04:57:11,346 root INFO [Epoch 10, Batch=299] Train: loss=51.9013, wer=0.4685, lr=0.0006733333333333334
04:58:38,443 root INFO [Epoch 10, Batch=399] Train: loss=68.7693, wer=0.4755, lr=0.0006766666666666668
05:00:05,212 root INFO [Epoch 10, Batch=499] Train: loss=62.0201, wer=0.4595, lr=0.00068
05:01:31,881 root INFO [Epoch 10, Batch=599] Train: loss=68.9198, wer=0.5019, lr=0.0006833333333333334
05:02:58,48 root INFO [Epoch 10, Batch=699] Train: loss=73.1136, wer=0.4888, lr=0.0006866666666666668
05:04:24,858 root INFO [Epoch 10, Batch=799] Train: loss=71.3535, wer=0.5415, lr=0.0006900000000000001
05:05:51,17 root INFO [Epoch 10, Batch=899] Train: loss=55.2710, wer=0.4584, lr=0.0006933333333333335
05:07:17,487 root INFO [Epoch 10, Batch=999] Train: loss=31.9213, wer=0.5243, lr=0.0006966666666666667
05:08:44,218 root INFO [Epoch 10, Batch=1099] Train: loss=70.5878, wer=0.5041, lr=0.0007000000000000001
05:10:11,42 root INFO [Epoch 10, Batch=1199] Train: loss=57.4841, wer=0.5000, lr=0.0007033333333333334
05:11:37,525 root INFO [Epoch 10, Batch=1299] Train: loss=56.7205, wer=0.4506, lr=0.0007066666666666667
05:13:04,406 root INFO [Epoch 10, Batch=1399] Train: loss=58.4156, wer=0.4598, lr=0.0007100000000000001
05:14:30,607 root INFO [Epoch 10, Batch=1499] Train: loss=59.2369, wer=0.4779, lr=0.0007133333333333334
05:15:57,526 root INFO [Epoch 10, Batch=1599] Train: loss=52.0467, wer=0.4825, lr=0.0007166666666666668
05:17:23,957 root INFO [Epoch 10, Batch=1699] Train: loss=46.9811, wer=0.5096, lr=0.00072
05:18:50,683 root INFO [Epoch 10, Batch=1799] Train: loss=54.9918, wer=0.4616, lr=0.0007233333333333334
05:20:17,136 root INFO [Epoch 10, Batch=1899] Train: loss=74.0014, wer=0.5382, lr=0.0007266666666666668
05:21:35,6 root INFO Start to validate epoch 10
05:22:00,871 root INFO Epoch 10, lr=0.0007297333333333335 | Train: loss=59.5229, wer=0.4819 | Val: loss=49.6034, wer=0.3217 | Time: this epoch 1750.07s, elapsed 18023.24s
05:22:01,159 root INFO [info] Save model after epoch 10

05:22:01,159 root INFO Start to train epoch 11
05:23:28,226 root INFO [Epoch 11, Batch=99] Train: loss=54.2769, wer=0.4450, lr=0.000733
05:24:55,303 root INFO [Epoch 11, Batch=199] Train: loss=18.6913, wer=0.5426, lr=0.0007363333333333334
05:26:21,808 root INFO [Epoch 11, Batch=299] Train: loss=49.3394, wer=0.4540, lr=0.0007396666666666667
05:27:48,764 root INFO [Epoch 11, Batch=399] Train: loss=59.2429, wer=0.4552, lr=0.0007430000000000001
05:29:15,80 root INFO [Epoch 11, Batch=499] Train: loss=81.0801, wer=0.5180, lr=0.0007463333333333334
05:30:41,569 root INFO [Epoch 11, Batch=599] Train: loss=52.5864, wer=0.4455, lr=0.0007496666666666667
05:32:08,87 root INFO [Epoch 11, Batch=699] Train: loss=28.7364, wer=0.5184, lr=0.0007530000000000001
05:33:34,924 root INFO [Epoch 11, Batch=799] Train: loss=79.1575, wer=0.5023, lr=0.0007563333333333334
05:35:01,270 root INFO [Epoch 11, Batch=899] Train: loss=37.3889, wer=0.4987, lr=0.0007596666666666667
05:36:28,241 root INFO [Epoch 11, Batch=999] Train: loss=47.9888, wer=0.4006, lr=0.0007630000000000001
05:37:54,851 root INFO [Epoch 11, Batch=1099] Train: loss=47.4819, wer=0.4155, lr=0.0007663333333333334
05:39:21,558 root INFO [Epoch 11, Batch=1199] Train: loss=66.6384, wer=0.4772, lr=0.0007696666666666668
05:40:48,140 root INFO [Epoch 11, Batch=1299] Train: loss=32.0376, wer=0.4969, lr=0.000773
05:42:15,152 root INFO [Epoch 11, Batch=1399] Train: loss=61.7292, wer=0.4714, lr=0.0007763333333333334
05:43:41,801 root INFO [Epoch 11, Batch=1499] Train: loss=75.0360, wer=0.4735, lr=0.0007796666666666667
05:45:08,794 root INFO [Epoch 11, Batch=1599] Train: loss=54.3740, wer=0.4653, lr=0.0007830000000000001
05:46:35,151 root INFO [Epoch 11, Batch=1699] Train: loss=57.0211, wer=0.4339, lr=0.0007863333333333334
05:48:01,993 root INFO [Epoch 11, Batch=1799] Train: loss=66.5321, wer=0.5000, lr=0.0007896666666666667
05:49:28,925 root INFO [Epoch 11, Batch=1899] Train: loss=78.7040, wer=0.4930, lr=0.0007930000000000001
05:50:46,862 root INFO Start to validate epoch 11
05:51:12,547 root INFO Epoch 11, lr=0.0007960666666666668 | Train: loss=56.8658, wer=0.4614 | Val: loss=46.9859, wer=0.3062 | Time: this epoch 1751.39s, elapsed 19774.92s
05:51:12,809 root INFO [info] Save model after epoch 11

05:51:12,809 root INFO Start to train epoch 12
05:52:40,490 root INFO [Epoch 12, Batch=99] Train: loss=30.6218, wer=0.4128, lr=0.0007993333333333334
05:54:07,57 root INFO [Epoch 12, Batch=199] Train: loss=50.2225, wer=0.4351, lr=0.0008026666666666668
05:55:34,217 root INFO [Epoch 12, Batch=299] Train: loss=69.5209, wer=0.4841, lr=0.0008060000000000002
05:57:01,67 root INFO [Epoch 12, Batch=399] Train: loss=68.1669, wer=0.5061, lr=0.0008093333333333335
05:58:27,553 root INFO [Epoch 12, Batch=499] Train: loss=45.5886, wer=0.3894, lr=0.0008126666666666668
05:59:54,5 root INFO [Epoch 12, Batch=599] Train: loss=70.5492, wer=0.4705, lr=0.0008160000000000001
06:01:20,910 root INFO [Epoch 12, Batch=699] Train: loss=51.0322, wer=0.4377, lr=0.0008193333333333335
06:02:47,654 root INFO [Epoch 12, Batch=799] Train: loss=65.3035, wer=0.4551, lr=0.0008226666666666669
06:04:14,575 root INFO [Epoch 12, Batch=899] Train: loss=39.5149, wer=0.4437, lr=0.0008260000000000001
06:05:41,244 root INFO [Epoch 12, Batch=999] Train: loss=66.3469, wer=0.4500, lr=0.0008293333333333335
06:07:07,884 root INFO [Epoch 12, Batch=1099] Train: loss=51.3742, wer=0.4358, lr=0.0008326666666666668
06:08:34,372 root INFO [Epoch 12, Batch=1199] Train: loss=54.0467, wer=0.4562, lr=0.0008360000000000002
06:10:01,287 root INFO [Epoch 12, Batch=1299] Train: loss=49.6953, wer=0.4377, lr=0.0008393333333333334
06:11:27,886 root INFO [Epoch 12, Batch=1399] Train: loss=60.0786, wer=0.4298, lr=0.0008426666666666668
06:12:55,29 root INFO [Epoch 12, Batch=1499] Train: loss=73.2600, wer=0.4838, lr=0.0008460000000000002
06:14:22,205 root INFO [Epoch 12, Batch=1599] Train: loss=50.1912, wer=0.4104, lr=0.0008493333333333335
06:15:48,846 root INFO [Epoch 12, Batch=1699] Train: loss=58.3157, wer=0.4446, lr=0.0008526666666666668
06:17:15,786 root INFO [Epoch 12, Batch=1799] Train: loss=50.9561, wer=0.4107, lr=0.0008560000000000001
06:18:42,325 root INFO [Epoch 12, Batch=1899] Train: loss=43.9108, wer=0.3810, lr=0.0008593333333333335
06:20:01,168 root INFO Start to validate epoch 12
06:20:27,14 root INFO Epoch 12, lr=0.0008624 | Train: loss=54.7840, wer=0.4458 | Val: loss=44.1487, wer=0.2899 | Time: this epoch 1754.21s, elapsed 21529.39s
06:20:27,269 root INFO [info] Save model after epoch 12

06:20:27,270 root INFO Start to train epoch 13
06:21:54,785 root INFO [Epoch 13, Batch=99] Train: loss=46.1542, wer=0.3914, lr=0.0008656666666666668
06:23:22,257 root INFO [Epoch 13, Batch=199] Train: loss=62.1885, wer=0.4552, lr=0.0008690000000000001
06:24:48,676 root INFO [Epoch 13, Batch=299] Train: loss=59.5320, wer=0.4288, lr=0.0008723333333333335
06:26:15,561 root INFO [Epoch 13, Batch=399] Train: loss=50.1472, wer=0.3791, lr=0.0008756666666666667
06:27:42,920 root INFO [Epoch 13, Batch=499] Train: loss=69.6613, wer=0.4823, lr=0.0008790000000000001
06:29:09,540 root INFO [Epoch 13, Batch=599] Train: loss=50.8357, wer=0.4360, lr=0.0008823333333333335
06:30:36,95 root INFO [Epoch 13, Batch=699] Train: loss=15.1359, wer=0.5397, lr=0.0008856666666666668
06:32:02,706 root INFO [Epoch 13, Batch=799] Train: loss=40.0755, wer=0.3573, lr=0.0008890000000000001
06:33:29,586 root INFO [Epoch 13, Batch=899] Train: loss=53.1226, wer=0.4569, lr=0.0008923333333333334
06:34:56,212 root INFO [Epoch 13, Batch=999] Train: loss=60.1510, wer=0.4522, lr=0.0008956666666666668
06:36:23,427 root INFO [Epoch 13, Batch=1099] Train: loss=68.0700, wer=0.4554, lr=0.0008990000000000002
06:37:49,857 root INFO [Epoch 13, Batch=1199] Train: loss=49.2571, wer=0.4229, lr=0.0009023333333333334
06:39:16,921 root INFO [Epoch 13, Batch=1299] Train: loss=21.0217, wer=0.5282, lr=0.0009056666666666668
06:40:43,962 root INFO [Epoch 13, Batch=1399] Train: loss=64.0911, wer=0.4180, lr=0.0009090000000000001
06:42:11,197 root INFO [Epoch 13, Batch=1499] Train: loss=52.7270, wer=0.4055, lr=0.0009123333333333335
06:43:38,121 root INFO [Epoch 13, Batch=1599] Train: loss=49.8611, wer=0.3900, lr=0.0009156666666666668
06:45:05,198 root INFO [Epoch 13, Batch=1699] Train: loss=50.4729, wer=0.4371, lr=0.0009190000000000001
06:46:32,37 root INFO [Epoch 13, Batch=1799] Train: loss=50.6519, wer=0.3619, lr=0.0009223333333333335
06:47:59,175 root INFO [Epoch 13, Batch=1899] Train: loss=52.0117, wer=0.4070, lr=0.0009256666666666668
06:49:17,419 root INFO Start to validate epoch 13
06:49:43,748 root INFO Epoch 13, lr=0.0009287333333333335 | Train: loss=53.4138, wer=0.4343 | Val: loss=43.0704, wer=0.2826 | Time: this epoch 1756.48s, elapsed 23286.12s
06:49:44,9 root INFO [info] Save model after epoch 13

06:49:44,9 root INFO Start to train epoch 14
06:51:11,230 root INFO [Epoch 14, Batch=99] Train: loss=49.1784, wer=0.3717, lr=0.0009320000000000001
06:52:38,659 root INFO [Epoch 14, Batch=199] Train: loss=54.0078, wer=0.4294, lr=0.0009353333333333335
06:54:05,361 root INFO [Epoch 14, Batch=299] Train: loss=53.6843, wer=0.4492, lr=0.0009386666666666668
06:55:32,374 root INFO [Epoch 14, Batch=399] Train: loss=65.1121, wer=0.4522, lr=0.0009420000000000001
06:56:59,165 root INFO [Epoch 14, Batch=499] Train: loss=48.3208, wer=0.4214, lr=0.0009453333333333334
06:58:26,305 root INFO [Epoch 14, Batch=599] Train: loss=47.9116, wer=0.4201, lr=0.0009486666666666668
06:59:53,220 root INFO [Epoch 14, Batch=699] Train: loss=51.1642, wer=0.3607, lr=0.000952
07:01:20,135 root INFO [Epoch 14, Batch=799] Train: loss=45.5433, wer=0.4368, lr=0.0009553333333333334
07:02:46,699 root INFO [Epoch 14, Batch=899] Train: loss=46.0629, wer=0.4103, lr=0.0009586666666666668
07:04:12,730 root INFO [Epoch 14, Batch=999] Train: loss=93.1203, wer=0.5120, lr=0.0009620000000000001
07:05:38,58 root INFO [Epoch 14, Batch=1099] Train: loss=60.3045, wer=0.4661, lr=0.0009653333333333335
07:07:03,740 root INFO [Epoch 14, Batch=1199] Train: loss=64.8782, wer=0.4309, lr=0.0009686666666666667
07:08:29,562 root INFO [Epoch 14, Batch=1299] Train: loss=49.1627, wer=0.3766, lr=0.0009720000000000001
07:09:55,81 root INFO [Epoch 14, Batch=1399] Train: loss=43.2112, wer=0.3749, lr=0.0009753333333333335
07:11:20,374 root INFO [Epoch 14, Batch=1499] Train: loss=57.9939, wer=0.4441, lr=0.0009786666666666667
07:12:46,547 root INFO [Epoch 14, Batch=1599] Train: loss=67.1603, wer=0.4250, lr=0.000982
07:14:12,648 root INFO [Epoch 14, Batch=1699] Train: loss=60.6640, wer=0.4368, lr=0.0009853333333333335
07:15:37,790 root INFO [Epoch 14, Batch=1799] Train: loss=40.7478, wer=0.3780, lr=0.0009886666666666668
07:17:03,572 root INFO [Epoch 14, Batch=1899] Train: loss=48.2487, wer=0.4004, lr=0.000992
07:18:20,414 root INFO Start to validate epoch 14
07:18:45,757 root INFO Epoch 14, lr=0.0009950666666666667 | Train: loss=52.1072, wer=0.4241 | Val: loss=41.7036, wer=0.2722 | Time: this epoch 1741.75s, elapsed 25028.13s
07:18:45,999 root INFO [info] Save model after epoch 14

07:18:45,999 root INFO Start to train epoch 15
07:20:12,378 root INFO [Epoch 15, Batch=99] Train: loss=51.4326, wer=0.4097, lr=0.0009983333333333335
07:21:37,528 root INFO [Epoch 15, Batch=199] Train: loss=43.9897, wer=0.4380, lr=0.0009991677068886809
07:23:03,338 root INFO [Epoch 15, Batch=299] Train: loss=52.9867, wer=0.4243, lr=0.000997509336107633
07:24:28,577 root INFO [Epoch 15, Batch=399] Train: loss=59.6897, wer=0.4155, lr=0.0009958591954639384
07:25:54,495 root INFO [Epoch 15, Batch=499] Train: loss=50.9997, wer=0.3610, lr=0.000994217217108047
07:27:20,164 root INFO [Epoch 15, Batch=599] Train: loss=58.1478, wer=0.4258, lr=0.0009925833339709303
07:28:46,97 root INFO [Epoch 15, Batch=699] Train: loss=60.0507, wer=0.4482, lr=0.0009909574797525764
07:30:11,462 root INFO [Epoch 15, Batch=799] Train: loss=40.2586, wer=0.4413, lr=0.000989339588910689
07:31:36,854 root INFO [Epoch 15, Batch=899] Train: loss=59.2416, wer=0.3973, lr=0.0009877295966495897
07:33:02,319 root INFO [Epoch 15, Batch=999] Train: loss=61.3285, wer=0.4031, lr=0.0009861274389093197
07:34:26,791 root INFO [Epoch 15, Batch=1099] Train: loss=48.9551, wer=0.3620, lr=0.0009845330523549332
07:35:51,862 root INFO [Epoch 15, Batch=1199] Train: loss=64.4668, wer=0.4517, lr=0.0009829463743659809
07:37:16,760 root INFO [Epoch 15, Batch=1299] Train: loss=41.8781, wer=0.3354, lr=0.0009813673430261814
07:38:42,51 root INFO [Epoch 15, Batch=1399] Train: loss=60.7989, wer=0.3987, lr=0.0009797958971132714
07:40:06,911 root INFO [Epoch 15, Batch=1499] Train: loss=48.3717, wer=0.3831, lr=0.000978231976089037
07:41:32,434 root INFO [Epoch 15, Batch=1599] Train: loss=44.9741, wer=0.3605, lr=0.000976675520089518
07:42:57,154 root INFO [Epoch 15, Batch=1699] Train: loss=58.5729, wer=0.4024, lr=0.0009751264699153826
07:44:22,252 root INFO [Epoch 15, Batch=1799] Train: loss=44.7945, wer=0.3617, lr=0.0009735847670224709
07:45:46,934 root INFO [Epoch 15, Batch=1899] Train: loss=52.4869, wer=0.3862, lr=0.0009720503535125018
07:47:03,362 root INFO Start to validate epoch 15
07:47:28,349 root INFO Epoch 15, lr=0.0009706450818307541 | Train: loss=50.4307, wer=0.4109 | Val: loss=40.1863, wer=0.2655 | Time: this epoch 1722.35s, elapsed 26750.72s
07:47:28,615 root INFO [info] Save model after epoch 15

07:47:28,615 root INFO Start to train epoch 16
07:48:54,608 root INFO [Epoch 16, Batch=99] Train: loss=49.1396, wer=0.3898, lr=0.0009691548455174002
07:50:19,798 root INFO [Epoch 16, Batch=199] Train: loss=40.7904, wer=0.3577, lr=0.0009676412496452295
07:51:44,785 root INFO [Epoch 16, Batch=299] Train: loss=46.8420, wer=0.3971, lr=0.0009661347233545834
07:53:09,533 root INFO [Epoch 16, Batch=399] Train: loss=40.4849, wer=0.3221, lr=0.0009646352117828866
07:54:34,767 root INFO [Epoch 16, Batch=499] Train: loss=51.6911, wer=0.3890, lr=0.0009631426606617745
07:55:59,444 root INFO [Epoch 16, Batch=599] Train: loss=46.2856, wer=0.4035, lr=0.0009616570163088448
07:57:25,144 root INFO [Epoch 16, Batch=699] Train: loss=40.5143, wer=0.3361, lr=0.0009601782256195477
07:58:49,526 root INFO [Epoch 16, Batch=799] Train: loss=39.9384, wer=0.4305, lr=0.0009587062360592132
08:00:14,349 root INFO [Epoch 16, Batch=899] Train: loss=49.6529, wer=0.4046, lr=0.0009572409956552118
08:01:39,284 root INFO [Epoch 16, Batch=999] Train: loss=52.8459, wer=0.4231, lr=0.0009557824529892479
08:03:04,373 root INFO [Epoch 16, Batch=1099] Train: loss=52.3610, wer=0.3874, lr=0.0009543305571897805
08:04:29,650 root INFO [Epoch 16, Batch=1199] Train: loss=43.1425, wer=0.3585, lr=0.0009528852579245708
08:05:54,307 root INFO [Epoch 16, Batch=1299] Train: loss=38.6392, wer=0.3541, lr=0.0009514465053933544
08:07:19,293 root INFO [Epoch 16, Batch=1399] Train: loss=49.9918, wer=0.3846, lr=0.0009500142503206331
08:08:43,828 root INFO [Epoch 16, Batch=1499] Train: loss=51.4604, wer=0.3656, lr=0.000948588443948587
08:10:09,223 root INFO [Epoch 16, Batch=1599] Train: loss=46.7398, wer=0.3976, lr=0.000947169038030103
08:11:33,971 root INFO [Epoch 16, Batch=1699] Train: loss=25.6458, wer=0.4119, lr=0.000945755984821918
08:12:59,66 root INFO [Epoch 16, Batch=1799] Train: loss=35.2578, wer=0.3456, lr=0.0009443492370778727
08:14:23,798 root INFO [Epoch 16, Batch=1899] Train: loss=37.4095, wer=0.4183, lr=0.0009429487480422778
08:15:40,605 root INFO Start to validate epoch 16
08:16:05,876 root INFO Epoch 16, lr=0.000941665786035186 | Train: loss=48.5967, wer=0.3968 | Val: loss=38.4083, wer=0.2489 | Time: this epoch 1717.26s, elapsed 28468.25s
08:16:06,114 root INFO [info] Save model after epoch 16

08:16:06,114 root INFO Start to train epoch 17
08:17:31,707 root INFO [Epoch 17, Batch=99] Train: loss=61.0470, wer=0.4191, lr=0.0009403048962790747
08:18:56,813 root INFO [Epoch 17, Batch=199] Train: loss=47.2113, wer=0.3724, lr=0.0009389222975364092
08:20:21,471 root INFO [Epoch 17, Batch=299] Train: loss=10.9420, wer=0.5765, lr=0.0009375457797202217
08:21:46,925 root INFO [Epoch 17, Batch=399] Train: loss=47.5406, wer=0.3183, lr=0.0009361752983857377
08:23:12,138 root INFO [Epoch 17, Batch=499] Train: loss=41.1451, wer=0.3692, lr=0.0009348108095416351
08:24:37,135 root INFO [Epoch 17, Batch=599] Train: loss=48.6895, wer=0.4078, lr=0.0009334522696441135
08:26:02,304 root INFO [Epoch 17, Batch=699] Train: loss=38.4132, wer=0.3456, lr=0.0009320996355910583
08:27:27,448 root INFO [Epoch 17, Batch=799] Train: loss=43.4878, wer=0.3416, lr=0.0009307528647162961
08:28:52,334 root INFO [Epoch 17, Batch=899] Train: loss=55.5743, wer=0.3999, lr=0.000929411914783943
08:30:17,234 root INFO [Epoch 17, Batch=999] Train: loss=47.5943, wer=0.3528, lr=0.000928076743982841
08:31:42,40 root INFO [Epoch 17, Batch=1099] Train: loss=50.6872, wer=0.3744, lr=0.0009267473109210818
08:33:06,848 root INFO [Epoch 17, Batch=1199] Train: loss=44.5531, wer=0.3598, lr=0.0009254235746206181
08:34:31,758 root INFO [Epoch 17, Batch=1299] Train: loss=35.0659, wer=0.3291, lr=0.000924105494511958
08:35:57,49 root INFO [Epoch 17, Batch=1399] Train: loss=56.3275, wer=0.3723, lr=0.0009227930304289428
08:37:21,702 root INFO [Epoch 17, Batch=1499] Train: loss=41.0252, wer=0.3501, lr=0.0009214861426036057
08:38:47,33 root INFO [Epoch 17, Batch=1599] Train: loss=52.0052, wer=0.3640, lr=0.0009201847916611116
08:40:11,918 root INFO [Epoch 17, Batch=1699] Train: loss=55.0370, wer=0.3779, lr=0.0009188889386147731
08:41:37,228 root INFO [Epoch 17, Batch=1799] Train: loss=28.3745, wer=0.4209, lr=0.0009175985448611468
08:43:02,371 root INFO [Epoch 17, Batch=1899] Train: loss=39.6107, wer=0.3522, lr=0.0009163135721752009
08:44:18,979 root INFO Start to validate epoch 17
08:44:44,305 root INFO Epoch 17, lr=0.0009151361526458867 | Train: loss=47.3270, wer=0.3853 | Val: loss=37.9854, wer=0.2450 | Time: this epoch 1718.19s, elapsed 30186.68s
08:44:44,551 root INFO [info] Save model after epoch 17

08:44:44,551 root INFO Start to train epoch 18
08:46:10,754 root INFO [Epoch 18, Batch=99] Train: loss=43.0152, wer=0.3392, lr=0.0009138869238460727
08:47:35,889 root INFO [Epoch 18, Batch=199] Train: loss=61.6089, wer=0.3912, lr=0.0009126174595246389
08:49:00,868 root INFO [Epoch 18, Batch=299] Train: loss=55.7933, wer=0.3806, lr=0.0009113532707206116
08:50:25,614 root INFO [Epoch 18, Batch=399] Train: loss=19.1788, wer=0.4050, lr=0.0009100943209957855
08:51:50,895 root INFO [Epoch 18, Batch=499] Train: loss=47.3717, wer=0.3575, lr=0.0009088405742633374
08:53:15,786 root INFO [Epoch 18, Batch=599] Train: loss=46.0537, wer=0.3719, lr=0.0009075919947834808
08:54:41,353 root INFO [Epoch 18, Batch=699] Train: loss=63.8041, wer=0.4064, lr=0.0009063485471591867
08:56:09,426 root INFO [Epoch 18, Batch=799] Train: loss=62.1164, wer=0.4171, lr=0.0009051101963319694
08:57:34,133 root INFO [Epoch 18, Batch=899] Train: loss=24.3504, wer=0.4096, lr=0.0009038769075777341
08:58:59,351 root INFO [Epoch 18, Batch=999] Train: loss=53.4482, wer=0.3506, lr=0.0009026486465026879
09:00:24,419 root INFO [Epoch 18, Batch=1099] Train: loss=52.7018, wer=0.4111, lr=0.0009014253790393105
09:01:49,927 root INFO [Epoch 18, Batch=1199] Train: loss=50.2888, wer=0.3641, lr=0.0009002070714423868
09:03:14,754 root INFO [Epoch 18, Batch=1299] Train: loss=42.7975, wer=0.3676, lr=0.0008989936902850953
09:04:39,988 root INFO [Epoch 18, Batch=1399] Train: loss=49.3227, wer=0.4191, lr=0.0008977852024551573
09:06:05,11 root INFO [Epoch 18, Batch=1499] Train: loss=36.9237, wer=0.3512, lr=0.0008965815751510409
09:07:29,569 root INFO [Epoch 18, Batch=1599] Train: loss=21.7575, wer=0.4192, lr=0.0008953827758782223
09:08:54,621 root INFO [Epoch 18, Batch=1699] Train: loss=42.1414, wer=0.3250, lr=0.0008941887724455003
09:10:19,295 root INFO [Epoch 18, Batch=1799] Train: loss=30.9716, wer=0.3794, lr=0.0008929995329613665
09:11:44,464 root INFO [Epoch 18, Batch=1899] Train: loss=42.3236, wer=0.3626, lr=0.0008918150258304268
09:13:01,198 root INFO Start to validate epoch 18
09:13:26,180 root INFO Epoch 18, lr=0.0008907294319699263 | Train: loss=45.5678, wer=0.3719 | Val: loss=35.3658, wer=0.2285 | Time: this epoch 1721.63s, elapsed 31908.55s
09:13:26,417 root INFO [info] Save model after epoch 18

09:13:26,417 root INFO Start to train epoch 19
09:14:51,987 root INFO [Epoch 19, Batch=99] Train: loss=39.0609, wer=0.3714, lr=0.0008895773880367305
09:16:17,330 root INFO [Epoch 19, Batch=199] Train: loss=46.5174, wer=0.3876, lr=0.0008884064287453961
09:17:41,983 root INFO [Epoch 19, Batch=299] Train: loss=42.2210, wer=0.3504, lr=0.0008872400813490358
09:19:07,45 root INFO [Epoch 19, Batch=399] Train: loss=34.5365, wer=0.3098, lr=0.0008860783156532042
09:20:31,905 root INFO [Epoch 19, Batch=499] Train: loss=45.1190, wer=0.3594, lr=0.0008849211017394925
09:21:56,875 root INFO [Epoch 19, Batch=599] Train: loss=42.6496, wer=0.3678, lr=0.0008837684099622929
09:23:21,691 root INFO [Epoch 19, Batch=699] Train: loss=36.4911, wer=0.3210, lr=0.0008826202109456084
09:24:46,386 root INFO [Epoch 19, Batch=799] Train: loss=66.9673, wer=0.4649, lr=0.0008814764755799085
09:26:10,873 root INFO [Epoch 19, Batch=899] Train: loss=54.6698, wer=0.3610, lr=0.0008803371750190297
09:27:36,217 root INFO [Epoch 19, Batch=999] Train: loss=20.5135, wer=0.4047, lr=0.0008792022806771196
09:29:01,101 root INFO [Epoch 19, Batch=1099] Train: loss=44.4423, wer=0.3639, lr=0.0008780717642256245
09:30:26,430 root INFO [Epoch 19, Batch=1199] Train: loss=37.3669, wer=0.2748, lr=0.0008769455975903197
09:31:51,722 root INFO [Epoch 19, Batch=1299] Train: loss=43.2284, wer=0.2984, lr=0.0008758237529483807
09:33:17,1 root INFO [Epoch 19, Batch=1399] Train: loss=38.9766, wer=0.2900, lr=0.0008747062027254966
09:34:41,994 root INFO [Epoch 19, Batch=1499] Train: loss=56.7539, wer=0.3811, lr=0.0008735929195930235
09:36:07,106 root INFO [Epoch 19, Batch=1599] Train: loss=27.4038, wer=0.4296, lr=0.0008724838764651773
09:37:32,741 root INFO [Epoch 19, Batch=1699] Train: loss=44.8448, wer=0.3571, lr=0.0008713790464962657
09:38:57,291 root INFO [Epoch 19, Batch=1799] Train: loss=26.2819, wer=0.3749, lr=0.0008702784030779595
09:40:22,440 root INFO [Epoch 19, Batch=1899] Train: loss=45.3308, wer=0.3778, lr=0.0008691819198366006
09:41:38,581 root INFO Start to validate epoch 19
09:42:03,844 root INFO Epoch 19, lr=0.0008681768070462669 | Train: loss=44.4054, wer=0.3634 | Val: loss=35.2777, wer=0.2280 | Time: this epoch 1717.43s, elapsed 33626.22s
09:42:04,102 root INFO [info] Save model after epoch 19

09:42:04,102 root INFO Start to train epoch 20
09:43:29,998 root INFO [Epoch 20, Batch=99] Train: loss=46.6702, wer=0.3382, lr=0.0008671099695241201
09:44:54,981 root INFO [Epoch 20, Batch=199] Train: loss=45.5424, wer=0.3382, lr=0.0008660254037844387
09:46:19,846 root INFO [Epoch 20, Batch=299] Train: loss=51.4026, wer=0.3553, lr=0.000864944897557338
09:47:45,344 root INFO [Epoch 20, Batch=399] Train: loss=45.1997, wer=0.3495, lr=0.0008638684255813601
09:49:10,371 root INFO [Epoch 20, Batch=499] Train: loss=25.2506, wer=0.3407, lr=0.0008627959628145763
09:50:35,492 root INFO [Epoch 20, Batch=599] Train: loss=67.0831, wer=0.4466, lr=0.0008617274844321392
09:52:00,567 root INFO [Epoch 20, Batch=699] Train: loss=23.5408, wer=0.3575, lr=0.0008606629658238705
09:53:25,764 root INFO [Epoch 20, Batch=799] Train: loss=35.9990, wer=0.3072, lr=0.0008596023825918793
09:54:51,2 root INFO [Epoch 20, Batch=899] Train: loss=35.1681, wer=0.2869, lr=0.0008585457105482137
09:56:16,147 root INFO [Epoch 20, Batch=999] Train: loss=49.1403, wer=0.3661, lr=0.0008574929257125442
09:57:41,620 root INFO [Epoch 20, Batch=1099] Train: loss=51.7640, wer=0.3301, lr=0.0008564440043098775
09:59:06,427 root INFO [Epoch 20, Batch=1199] Train: loss=40.6958, wer=0.3385, lr=0.0008553989227683017
10:00:31,710 root INFO [Epoch 20, Batch=1299] Train: loss=33.5166, wer=0.3658, lr=0.000854357657716761
10:01:56,737 root INFO [Epoch 20, Batch=1399] Train: loss=41.9327, wer=0.3372, lr=0.0008533201859828615
10:03:22,193 root INFO [Epoch 20, Batch=1499] Train: loss=59.6104, wer=0.3417, lr=0.0008522864845907042
10:04:47,323 root INFO [Epoch 20, Batch=1599] Train: loss=47.6188, wer=0.3498, lr=0.0008512565307587486
10:06:12,183 root INFO [Epoch 20, Batch=1699] Train: loss=53.6238, wer=0.3261, lr=0.0008502303018977037
10:07:37,113 root INFO [Epoch 20, Batch=1799] Train: loss=48.4414, wer=0.3677, lr=0.0008492077756084469
10:09:02,324 root INFO [Epoch 20, Batch=1899] Train: loss=38.2130, wer=0.3192, lr=0.000848188929679971
10:10:18,921 root INFO Start to validate epoch 20
10:10:43,836 root INFO Epoch 20, lr=0.0008472548229835655 | Train: loss=43.5119, wer=0.3556 | Val: loss=35.0902, wer=0.2247 | Time: this epoch 1719.73s, elapsed 35346.21s
10:10:44,82 root INFO [info] Save model after epoch 20

10:10:44,82 root INFO Start to train epoch 21
10:12:10,114 root INFO [Epoch 21, Batch=99] Train: loss=35.0857, wer=0.3236, lr=0.0008462631830759508
10:13:35,246 root INFO [Epoch 21, Batch=199] Train: loss=40.9569, wer=0.3220, lr=0.0008452548863005501
10:15:00,186 root INFO [Epoch 21, Batch=299] Train: loss=34.5453, wer=0.3335, lr=0.000844250185022725
10:16:25,735 root INFO [Epoch 21, Batch=399] Train: loss=36.1580, wer=0.3136, lr=0.0008432490579244356
10:17:51,460 root INFO [Epoch 21, Batch=499] Train: loss=51.3667, wer=0.3566, lr=0.0008422514838641793
10:19:15,870 root INFO [Epoch 21, Batch=599] Train: loss=39.9203, wer=0.3324, lr=0.0008412574418751147
10:20:41,112 root INFO [Epoch 21, Batch=699] Train: loss=22.1970, wer=0.3842, lr=0.0008402669111632112
10:22:06,162 root INFO [Epoch 21, Batch=799] Train: loss=55.0765, wer=0.3677, lr=0.0008392798711054216
10:23:31,121 root INFO [Epoch 21, Batch=899] Train: loss=41.9393, wer=0.3617, lr=0.0008382963012478793
10:24:56,153 root INFO [Epoch 21, Batch=999] Train: loss=39.6595, wer=0.3342, lr=0.000837316181304117
10:26:21,417 root INFO [Epoch 21, Batch=1099] Train: loss=59.4521, wer=0.4205, lr=0.0008363394911533108
10:27:46,780 root INFO [Epoch 21, Batch=1199] Train: loss=35.0016, wer=0.3280, lr=0.000835366210838544
10:29:11,895 root INFO [Epoch 21, Batch=1299] Train: loss=36.6788, wer=0.3483, lr=0.0008343963205650952
10:30:37,126 root INFO [Epoch 21, Batch=1399] Train: loss=51.8569, wer=0.3845, lr=0.000833429800698747
10:32:02,230 root INFO [Epoch 21, Batch=1499] Train: loss=34.6488, wer=0.3267, lr=0.0008324666317641164
10:33:27,641 root INFO [Epoch 21, Batch=1599] Train: loss=35.7638, wer=0.3668, lr=0.0008315067944430076
10:34:52,551 root INFO [Epoch 21, Batch=1699] Train: loss=32.7591, wer=0.2996, lr=0.000830550269572783
10:36:18,61 root INFO [Epoch 21, Batch=1799] Train: loss=34.2482, wer=0.3047, lr=0.0008295970381447578
10:37:42,595 root INFO [Epoch 21, Batch=1899] Train: loss=12.1179, wer=0.4378, lr=0.0008286470813026128
10:38:59,170 root INFO Start to validate epoch 21
10:39:24,304 root INFO Epoch 21, lr=0.000827775997039354 | Train: loss=42.3343, wer=0.3461 | Val: loss=33.2708, wer=0.2137 | Time: this epoch 1720.22s, elapsed 37066.68s
10:39:24,566 root INFO [info] Save model after epoch 21

10:39:24,566 root INFO Start to train epoch 22
10:40:50,491 root INFO [Epoch 22, Batch=99] Train: loss=53.4635, wer=0.3824, lr=0.0008268511179131138
10:42:15,658 root INFO [Epoch 22, Batch=199] Train: loss=24.5025, wer=0.3298, lr=0.0008259105521236186
10:43:41,175 root INFO [Epoch 22, Batch=299] Train: loss=55.9665, wer=0.3605, lr=0.0008249731888070387
10:45:06,543 root INFO [Epoch 22, Batch=399] Train: loss=44.1546, wer=0.3772, lr=0.0008240390098313654
10:46:31,755 root INFO [Epoch 22, Batch=499] Train: loss=48.8735, wer=0.3343, lr=0.0008231079972079914
10:47:56,911 root INFO [Epoch 22, Batch=599] Train: loss=25.2535, wer=0.3868, lr=0.0008221801330902554
10:49:21,924 root INFO [Epoch 22, Batch=699] Train: loss=33.7107, wer=0.3091, lr=0.0008212553997720053
10:50:47,239 root INFO [Epoch 22, Batch=799] Train: loss=48.6199, wer=0.3230, lr=0.0008203337796861791
10:52:12,2 root INFO [Epoch 22, Batch=899] Train: loss=66.2719, wer=0.4064, lr=0.0008194152554034033
10:53:37,237 root INFO [Epoch 22, Batch=999] Train: loss=45.3030, wer=0.3434, lr=0.0008184998096306081
10:55:02,8 root INFO [Epoch 22, Batch=1099] Train: loss=50.3143, wer=0.3691, lr=0.0008175874252096608
10:56:27,166 root INFO [Epoch 22, Batch=1199] Train: loss=26.7329, wer=0.3690, lr=0.000816678085116014
10:57:52,345 root INFO [Epoch 22, Batch=1299] Train: loss=46.0940, wer=0.3521, lr=0.0008157717724573726
10:59:17,77 root INFO [Epoch 22, Batch=1399] Train: loss=48.7577, wer=0.3550, lr=0.0008148684704723743
11:00:41,652 root INFO [Epoch 22, Batch=1499] Train: loss=37.0799, wer=0.3345, lr=0.0008139681625292893
11:02:06,833 root INFO [Epoch 22, Batch=1599] Train: loss=42.5669, wer=0.3029, lr=0.0008130708321247328
11:03:32,249 root INFO [Epoch 22, Batch=1699] Train: loss=35.8466, wer=0.3023, lr=0.000812176462882395
11:04:56,988 root INFO [Epoch 22, Batch=1799] Train: loss=51.5614, wer=0.3349, lr=0.0008112850385517859
11:06:21,976 root INFO [Epoch 22, Batch=1899] Train: loss=48.5247, wer=0.3687, lr=0.0008103965430069951
11:07:38,466 root INFO Start to validate epoch 22
11:08:03,325 root INFO Epoch 22, lr=0.0008095817000507518 | Train: loss=41.8599, wer=0.3408 | Val: loss=33.3836, wer=0.2142 | Time: this epoch 1718.76s, elapsed 38785.70s
11:08:03,325 root INFO Start to train epoch 23
11:09:29,478 root INFO [Epoch 23, Batch=99] Train: loss=37.8292, wer=0.3024, lr=0.0008087164130621127
11:10:54,747 root INFO [Epoch 23, Batch=199] Train: loss=43.0420, wer=0.3221, lr=0.0008078363209382318
11:12:19,892 root INFO [Epoch 23, Batch=299] Train: loss=39.8930, wer=0.3150, lr=0.0008069590958766495
11:13:44,909 root INFO [Epoch 23, Batch=399] Train: loss=48.1120, wer=0.3584, lr=0.0008060847223444575
11:15:10,135 root INFO [Epoch 23, Batch=499] Train: loss=41.2767, wer=0.2950, lr=0.0008052131849263062
11:16:35,682 root INFO [Epoch 23, Batch=599] Train: loss=43.5811, wer=0.3279, lr=0.000804344468323264
11:18:00,601 root INFO [Epoch 23, Batch=699] Train: loss=34.3982, wer=0.3051, lr=0.0008034785573516887
11:19:25,714 root INFO [Epoch 23, Batch=799] Train: loss=26.5143, wer=0.3229, lr=0.0008026154369421135
11:20:50,685 root INFO [Epoch 23, Batch=899] Train: loss=35.6572, wer=0.3376, lr=0.0008017550921381448
11:22:16,32 root INFO [Epoch 23, Batch=999] Train: loss=48.7270, wer=0.3243, lr=0.0008008975080953745
11:23:41,446 root INFO [Epoch 23, Batch=1099] Train: loss=50.6965, wer=0.3814, lr=0.0008000426700803035
11:25:06,25 root INFO [Epoch 23, Batch=1199] Train: loss=43.2216, wer=0.3534, lr=0.000799190563469279
11:26:30,685 root INFO [Epoch 23, Batch=1299] Train: loss=32.9470, wer=0.3508, lr=0.0007983411737474438
11:27:55,736 root INFO [Epoch 23, Batch=1399] Train: loss=35.7796, wer=0.3134, lr=0.0007974944865076975
11:29:21,325 root INFO [Epoch 23, Batch=1499] Train: loss=33.7866, wer=0.4020, lr=0.0007966504874496703
11:30:45,998 root INFO [Epoch 23, Batch=1599] Train: loss=31.5619, wer=0.2704, lr=0.0007958091623787091
11:32:11,252 root INFO [Epoch 23, Batch=1699] Train: loss=52.9608, wer=0.3475, lr=0.0007949704972048736
11:33:36,640 root INFO [Epoch 23, Batch=1799] Train: loss=40.3085, wer=0.3435, lr=0.0007941344779419465
11:35:01,595 root INFO [Epoch 23, Batch=1899] Train: loss=47.8544, wer=0.3359, lr=0.000793301090706453
11:36:18,621 root INFO Start to validate epoch 23
11:36:43,708 root INFO Epoch 23, lr=0.0007925366872072803 | Train: loss=41.0468, wer=0.3333 | Val: loss=32.9932, wer=0.2126 | Time: this epoch 1720.38s, elapsed 40506.08s
11:36:43,929 root INFO [info] Save model after epoch 23

11:36:43,930 root INFO Start to train epoch 24
11:38:10,101 root INFO [Epoch 24, Batch=99] Train: loss=40.7013, wer=0.3110, lr=0.0007917248569166928
11:39:34,987 root INFO [Epoch 24, Batch=199] Train: loss=39.9083, wer=0.3131, lr=0.0007908990249855558
11:41:00,427 root INFO [Epoch 24, Batch=299] Train: loss=33.4511, wer=0.3129, lr=0.000790075771899962
11:42:25,459 root INFO [Epoch 24, Batch=399] Train: loss=24.4062, wer=0.3018, lr=0.0007892550842660864
11:43:50,401 root INFO [Epoch 24, Batch=499] Train: loss=43.3047, wer=0.3199, lr=0.0007884369487872907
11:45:15,274 root INFO [Epoch 24, Batch=599] Train: loss=41.5436, wer=0.2997, lr=0.0007876213522632197
11:46:40,665 root INFO [Epoch 24, Batch=699] Train: loss=36.2771, wer=0.2950, lr=0.0007868082815889067
11:48:05,611 root INFO [Epoch 24, Batch=799] Train: loss=33.1098, wer=0.3265, lr=0.0007859977237538881
11:49:31,111 root INFO [Epoch 24, Batch=899] Train: loss=41.4930, wer=0.2883, lr=0.0007851896658413303
11:50:56,495 root INFO [Epoch 24, Batch=999] Train: loss=47.1469, wer=0.3275, lr=0.0007843840950271643
11:52:21,414 root INFO [Epoch 24, Batch=1099] Train: loss=51.4289, wer=0.3597, lr=0.0007835809985792311
11:53:46,811 root INFO [Epoch 24, Batch=1199] Train: loss=50.7622, wer=0.3160, lr=0.0007827803638564369
11:55:11,893 root INFO [Epoch 24, Batch=1299] Train: loss=36.0226, wer=0.3055, lr=0.0007819821783079168
11:56:37,15 root INFO [Epoch 24, Batch=1399] Train: loss=37.1197, wer=0.2943, lr=0.0007811864294722086
11:58:02,294 root INFO [Epoch 24, Batch=1499] Train: loss=40.2432, wer=0.3257, lr=0.000780393104976436
11:59:26,829 root INFO [Epoch 24, Batch=1599] Train: loss=40.9324, wer=0.3088, lr=0.0007796021925355
12:00:52,19 root INFO [Epoch 24, Batch=1699] Train: loss=39.1854, wer=0.3034, lr=0.0007788136799512808
12:02:16,723 root INFO [Epoch 24, Batch=1799] Train: loss=51.1754, wer=0.3693, lr=0.0007780275551118465
12:03:42,102 root INFO [Epoch 24, Batch=1899] Train: loss=41.9271, wer=0.3300, lr=0.0007772438059906724
12:04:58,630 root INFO Start to validate epoch 24
12:05:23,593 root INFO Epoch 24, lr=0.0007765248447662927 | Train: loss=40.3729, wer=0.3288 | Val: loss=32.0729, wer=0.2084 | Time: this epoch 1719.66s, elapsed 42225.96s
12:05:23,832 root INFO [info] Save model after epoch 24

12:05:23,832 root INFO Start to train epoch 25
12:06:49,997 root INFO [Epoch 25, Batch=99] Train: loss=58.6378, wer=0.3811, lr=0.0007757611850619663
12:08:15,574 root INFO [Epoch 25, Batch=199] Train: loss=34.0815, wer=0.2864, lr=0.0007749842582921286
12:09:40,561 root INFO [Epoch 25, Batch=299] Train: loss=35.0185, wer=0.3265, lr=0.0007742096611387638
12:11:06,133 root INFO [Epoch 25, Batch=399] Train: loss=42.8309, wer=0.3278, lr=0.0007734373819828304
12:12:37,421 root INFO [Epoch 25, Batch=499] Train: loss=48.3024, wer=0.3312, lr=0.0007726674092862559
12:14:02,623 root INFO [Epoch 25, Batch=599] Train: loss=24.7399, wer=0.3237, lr=0.0007718997315912126
12:15:27,804 root INFO [Epoch 25, Batch=699] Train: loss=43.0817, wer=0.3285, lr=0.0007711343375194018
12:16:53,768 root INFO [Epoch 25, Batch=799] Train: loss=54.9463, wer=0.3430, lr=0.0007703712157713455
12:18:18,760 root INFO [Epoch 25, Batch=899] Train: loss=36.3643, wer=0.3096, lr=0.0007696103551256852
12:19:44,122 root INFO [Epoch 25, Batch=999] Train: loss=40.3947, wer=0.2815, lr=0.0007688517444384896
12:21:09,792 root INFO [Epoch 25, Batch=1099] Train: loss=41.2947, wer=0.3041, lr=0.0007680953726425686
12:22:34,907 root INFO [Epoch 25, Batch=1199] Train: loss=41.4321, wer=0.3265, lr=0.0007673412287467955
12:24:00,280 root INFO [Epoch 25, Batch=1299] Train: loss=46.3421, wer=0.3153, lr=0.0007665893018354357
12:25:25,368 root INFO [Epoch 25, Batch=1399] Train: loss=62.9503, wer=0.3866, lr=0.0007658395810674837
12:26:50,440 root INFO [Epoch 25, Batch=1499] Train: loss=32.7668, wer=0.3062, lr=0.000765092055676006
12:28:15,543 root INFO [Epoch 25, Batch=1599] Train: loss=36.6572, wer=0.3157, lr=0.0007643467149674924
12:29:40,795 root INFO [Epoch 25, Batch=1699] Train: loss=43.1387, wer=0.3029, lr=0.0007636035483212126
12:31:05,858 root INFO [Epoch 25, Batch=1799] Train: loss=21.9032, wer=0.3558, lr=0.0007628625451885815
12:32:30,643 root INFO [Epoch 25, Batch=1899] Train: loss=44.0970, wer=0.3258, lr=0.0007621236950925295
12:33:47,233 root INFO Start to validate epoch 25
12:34:12,575 root INFO Epoch 25, lr=0.0007614458456192098 | Train: loss=39.5979, wer=0.3232 | Val: loss=32.2447, wer=0.2035 | Time: this epoch 1728.74s, elapsed 43954.95s
12:34:12,786 root INFO [info] Save model after epoch 25

12:34:12,786 root INFO Start to train epoch 26
12:35:39,54 root INFO [Epoch 26, Batch=99] Train: loss=59.8330, wer=0.3678, lr=0.0007607257743127308
12:37:04,207 root INFO [Epoch 26, Batch=199] Train: loss=35.9526, wer=0.2772, lr=0.000759993109427045
12:38:29,536 root INFO [Epoch 26, Batch=299] Train: loss=33.4071, wer=0.2911, lr=0.0007592625573890999
12:39:54,616 root INFO [Epoch 26, Batch=399] Train: loss=36.6397, wer=0.2935, lr=0.0007585341080633831
12:41:20,213 root INFO [Epoch 26, Batch=499] Train: loss=38.1005, wer=0.2741, lr=0.0007578077513823214
12:42:45,641 root INFO [Epoch 26, Batch=599] Train: loss=47.9989, wer=0.3623, lr=0.000757083477345696
12:44:11,363 root INFO [Epoch 26, Batch=699] Train: loss=39.6413, wer=0.3139, lr=0.0007563612760200645
12:45:36,935 root INFO [Epoch 26, Batch=799] Train: loss=45.9675, wer=0.3307, lr=0.0007556411375381887
12:47:03,353 root INFO [Epoch 26, Batch=899] Train: loss=38.1163, wer=0.2788, lr=0.0007549230520984677
12:48:30,433 root INFO [Epoch 26, Batch=999] Train: loss=27.0584, wer=0.3525, lr=0.0007542070099643787
12:49:57,56 root INFO [Epoch 26, Batch=1099] Train: loss=27.3712, wer=0.2754, lr=0.000753493001463922
12:51:23,916 root INFO [Epoch 26, Batch=1199] Train: loss=54.6747, wer=0.3593, lr=0.0007527810169890716
12:52:49,348 root INFO [Epoch 26, Batch=1299] Train: loss=52.0486, wer=0.3065, lr=0.0007520710469952335
12:54:16,265 root INFO [Epoch 26, Batch=1399] Train: loss=41.1104, wer=0.3292, lr=0.0007513630820007074
12:55:42,752 root INFO [Epoch 26, Batch=1499] Train: loss=44.1085, wer=0.3255, lr=0.000750657112586155
12:57:09,680 root INFO [Epoch 26, Batch=1599] Train: loss=57.8446, wer=0.3531, lr=0.0007499531293940736
12:58:36,750 root INFO [Epoch 26, Batch=1699] Train: loss=34.4872, wer=0.3108, lr=0.0007492511231282754
13:00:02,143 root INFO [Epoch 26, Batch=1799] Train: loss=27.0414, wer=0.3333, lr=0.0007485510845533718
13:01:28,960 root INFO [Epoch 26, Batch=1899] Train: loss=47.6990, wer=0.3724, lr=0.0007478530044942631
13:02:47,79 root INFO Start to validate epoch 26
13:03:12,965 root INFO Epoch 26, lr=0.0007472124927640488 | Train: loss=39.1923, wer=0.3183 | Val: loss=31.5958, wer=0.2013 | Time: this epoch 1740.18s, elapsed 45695.34s
13:03:13,218 root INFO [info] Save model after epoch 26

13:03:13,218 root INFO Start to train epoch 27
13:04:41,361 root INFO [Epoch 27, Batch=99] Train: loss=29.9621, wer=0.3081, lr=0.0007465320154942483
13:06:08,112 root INFO [Epoch 27, Batch=199] Train: loss=31.4473, wer=0.2860, lr=0.0007458395637957593
13:07:34,797 root INFO [Epoch 27, Batch=299] Train: loss=64.7472, wer=0.3811, lr=0.0007451490353968151
13:09:01,259 root INFO [Epoch 27, Batch=399] Train: loss=27.5272, wer=0.3102, lr=0.000744460421410527
13:10:27,370 root INFO [Epoch 27, Batch=499] Train: loss=30.4244, wer=0.2558, lr=0.0007437737130073882
13:11:54,298 root INFO [Epoch 27, Batch=599] Train: loss=33.7135, wer=0.3344, lr=0.0007430889014147992
13:13:20,679 root INFO [Epoch 27, Batch=699] Train: loss=25.5703, wer=0.3008, lr=0.0007424059779165966
13:14:47,157 root INFO [Epoch 27, Batch=799] Train: loss=46.5123, wer=0.3414, lr=0.0007417249338525871
13:16:14,59 root INFO [Epoch 27, Batch=899] Train: loss=42.0717, wer=0.3157, lr=0.0007410457606180866
13:17:39,862 root INFO [Epoch 27, Batch=999] Train: loss=42.9252, wer=0.3236, lr=0.0007403684496634634
13:19:06,273 root INFO [Epoch 27, Batch=1099] Train: loss=52.6394, wer=0.3607, lr=0.000739692992493686
13:20:32,525 root INFO [Epoch 27, Batch=1199] Train: loss=53.9153, wer=0.3398, lr=0.000739019380667876
13:21:59,159 root INFO [Epoch 27, Batch=1299] Train: loss=31.7241, wer=0.2689, lr=0.0007383476057988643
13:23:26,145 root INFO [Epoch 27, Batch=1399] Train: loss=33.1291, wer=0.2745, lr=0.0007376776595527539
13:24:52,780 root INFO [Epoch 27, Batch=1499] Train: loss=47.8773, wer=0.3452, lr=0.0007370095336484842
13:26:19,918 root INFO [Epoch 27, Batch=1599] Train: loss=45.4088, wer=0.3320, lr=0.0007363432198574016
13:27:48,435 root INFO [Epoch 27, Batch=1699] Train: loss=42.8091, wer=0.3317, lr=0.0007356787100028343
13:29:17,18 root INFO [Epoch 27, Batch=1799] Train: loss=33.1396, wer=0.2844, lr=0.00073501599595967
13:30:45,296 root INFO [Epoch 27, Batch=1899] Train: loss=43.0331, wer=0.3140, lr=0.0007343550696539396
13:32:05,210 root INFO Start to validate epoch 27
13:32:30,686 root INFO Epoch 27, lr=0.0007337485894843407 | Train: loss=38.7346, wer=0.3140 | Val: loss=31.7422, wer=0.1965 | Time: this epoch 1757.47s, elapsed 47453.06s
13:32:30,946 root INFO [info] Save model after epoch 27

13:32:30,946 root INFO Start to train epoch 28
13:34:00,483 root INFO [Epoch 28, Batch=99] Train: loss=40.8499, wer=0.2830, lr=0.000733104206195006
13:35:28,357 root INFO [Epoch 28, Batch=199] Train: loss=31.6889, wer=0.3245, lr=0.0007324484191363096
13:36:55,445 root INFO [Epoch 28, Batch=299] Train: loss=34.1639, wer=0.3165, lr=0.0007317943888075271
13:38:23,438 root INFO [Epoch 28, Batch=399] Train: loss=44.2914, wer=0.3409, lr=0.0007311421073793924
13:39:51,180 root INFO [Epoch 28, Batch=499] Train: loss=37.7909, wer=0.3232, lr=0.0007304915670714025
13:41:18,717 root INFO [Epoch 28, Batch=599] Train: loss=26.8001, wer=0.2662, lr=0.0007298427601514279
13:42:45,457 root INFO [Epoch 28, Batch=699] Train: loss=36.8571, wer=0.2928, lr=0.000729195678935326
13:44:12,470 root INFO [Epoch 28, Batch=799] Train: loss=44.0709, wer=0.3014, lr=0.0007285503157865601
13:45:40,433 root INFO [Epoch 28, Batch=899] Train: loss=54.7819, wer=0.3403, lr=0.0007279066631158194
13:47:07,250 root INFO [Epoch 28, Batch=999] Train: loss=18.8007, wer=0.2736, lr=0.0007272647133806452
13:48:33,211 root INFO [Epoch 28, Batch=1099] Train: loss=77.7759, wer=0.4157, lr=0.0007266244590850592
13:50:00,856 root INFO [Epoch 28, Batch=1199] Train: loss=31.6703, wer=0.2599, lr=0.000725985892779196
13:51:27,934 root INFO [Epoch 28, Batch=1299] Train: loss=52.8410, wer=0.3264, lr=0.0007253490070589393
13:52:55,298 root INFO [Epoch 28, Batch=1399] Train: loss=53.9032, wer=0.3375, lr=0.0007247137945655606
13:54:22,67 root INFO [Epoch 28, Batch=1499] Train: loss=43.9810, wer=0.2937, lr=0.0007240802479853629
13:55:50,581 root INFO [Epoch 28, Batch=1599] Train: loss=45.1109, wer=0.3178, lr=0.0007234483600493265
13:57:16,881 root INFO [Epoch 28, Batch=1699] Train: loss=32.1396, wer=0.2720, lr=0.0007228181235327592
13:58:44,247 root INFO [Epoch 28, Batch=1799] Train: loss=41.0186, wer=0.3429, lr=0.0007221895312549484
14:00:11,862 root INFO [Epoch 28, Batch=1899] Train: loss=49.8611, wer=0.3286, lr=0.0007215625760788187
14:01:29,544 root INFO Start to validate epoch 28
14:01:56,129 root INFO Epoch 28, lr=0.0007209872171060909 | Train: loss=37.9840, wer=0.3091 | Val: loss=30.6388, wer=0.1940 | Time: this epoch 1765.18s, elapsed 49218.50s
14:01:56,379 root INFO [info] Save model after epoch 28

14:01:56,379 root INFO Start to train epoch 29
14:03:24,35 root INFO [Epoch 29, Batch=99] Train: loss=22.3613, wer=0.3442, lr=0.000720375846087556
14:04:51,544 root INFO [Epoch 29, Batch=199] Train: loss=37.9337, wer=0.2765, lr=0.0007197535985445681
14:06:19,279 root INFO [Epoch 29, Batch=299] Train: loss=46.2754, wer=0.3099, lr=0.0007191329606792624
14:07:47,364 root INFO [Epoch 29, Batch=399] Train: loss=33.8996, wer=0.2931, lr=0.0007185139255635232
14:09:14,533 root INFO [Epoch 29, Batch=499] Train: loss=40.5817, wer=0.3024, lr=0.0007178964863109098
14:10:41,646 root INFO [Epoch 29, Batch=599] Train: loss=48.0337, wer=0.3221, lr=0.0007172806360763341
14:12:07,698 root INFO [Epoch 29, Batch=699] Train: loss=41.2715, wer=0.3165, lr=0.0007166663680557423
14:13:35,470 root INFO [Epoch 29, Batch=799] Train: loss=47.3263, wer=0.3525, lr=0.000716053675485799
14:15:02,786 root INFO [Epoch 29, Batch=899] Train: loss=41.4089, wer=0.3591, lr=0.000715442551643575
14:16:30,716 root INFO [Epoch 29, Batch=999] Train: loss=25.3252, wer=0.2909, lr=0.000714832989846236
14:17:57,493 root INFO [Epoch 29, Batch=1099] Train: loss=38.9840, wer=0.2868, lr=0.0007142249834507378
14:19:24,762 root INFO [Epoch 29, Batch=1199] Train: loss=57.5744, wer=0.3677, lr=0.0007136185258535202
14:20:52,46 root INFO [Epoch 29, Batch=1299] Train: loss=43.2720, wer=0.2884, lr=0.0007130136104902071
14:22:20,12 root INFO [Epoch 29, Batch=1399] Train: loss=38.7960, wer=0.3027, lr=0.0007124102308353074
14:23:46,562 root INFO [Epoch 29, Batch=1499] Train: loss=36.3603, wer=0.2949, lr=0.0007118083804019195
14:25:13,839 root INFO [Epoch 29, Batch=1599] Train: loss=42.0435, wer=0.2969, lr=0.0007112080527414388
14:26:40,926 root INFO [Epoch 29, Batch=1699] Train: loss=61.2459, wer=0.3763, lr=0.0007106092414432668
14:28:09,287 root INFO [Epoch 29, Batch=1799] Train: loss=35.0924, wer=0.2623, lr=0.000710011940134524
14:29:35,427 root INFO [Epoch 29, Batch=1899] Train: loss=47.9374, wer=0.3175, lr=0.0007094161424797653
14:30:54,225 root INFO Start to validate epoch 29
14:31:20,470 root INFO Epoch 29, lr=0.0007088693312496355 | Train: loss=37.5339, wer=0.3050 | Val: loss=30.7223, wer=0.1901 | Time: this epoch 1764.09s, elapsed 50982.84s
14:31:20,690 root INFO [info] Save model after epoch 29

14:31:20,690 root INFO Start to train epoch 30
14:32:48,115 root INFO [Epoch 30, Batch=99] Train: loss=44.4707, wer=0.3399, lr=0.0007082882469748285
14:34:15,822 root INFO [Epoch 30, Batch=199] Train: loss=44.5406, wer=0.3530, lr=0.0007076967744316076
14:35:43,419 root INFO [Epoch 30, Batch=299] Train: loss=33.2490, wer=0.2700, lr=0.0007071067811865476
14:37:09,833 root INFO [Epoch 30, Batch=399] Train: loss=36.4719, wer=0.2397, lr=0.0007065182610835989
14:40:41,531 root INFO [Epoch 30, Batch=499] Train: loss=32.4034, wer=0.2828, lr=0.0007059312080025176
14:42:09,72 root INFO [Epoch 30, Batch=599] Train: loss=33.3472, wer=0.3026, lr=0.0007053456158585983
14:43:37,975 root INFO [Epoch 30, Batch=699] Train: loss=42.1029, wer=0.3057, lr=0.0007047614786024094
14:45:06,759 root INFO [Epoch 30, Batch=799] Train: loss=46.1354, wer=0.3146, lr=0.0007041787902195304
14:46:35,594 root INFO [Epoch 30, Batch=899] Train: loss=41.7271, wer=0.2847, lr=0.0007035975447302918
14:48:04,493 root INFO [Epoch 30, Batch=999] Train: loss=51.9219, wer=0.3339, lr=0.0007030177361895179
14:49:33,142 root INFO [Epoch 30, Batch=1099] Train: loss=46.4132, wer=0.2956, lr=0.0007024393586862705
14:51:01,813 root INFO [Epoch 30, Batch=1199] Train: loss=32.0986, wer=0.2916, lr=0.0007018624063435964
14:52:30,544 root INFO [Epoch 30, Batch=1299] Train: loss=40.0128, wer=0.3091, lr=0.0007012868733182767
14:53:59,182 root INFO [Epoch 30, Batch=1399] Train: loss=22.3720, wer=0.3015, lr=0.0007007127538005777
14:55:27,377 root INFO [Epoch 30, Batch=1499] Train: loss=42.8767, wer=0.2924, lr=0.0007001400420140049
14:56:55,775 root INFO [Epoch 30, Batch=1599] Train: loss=33.0053, wer=0.2767, lr=0.0006995687322150593
14:58:24,1 root INFO [Epoch 30, Batch=1699] Train: loss=15.2306, wer=0.3799, lr=0.0006989988186929947
14:59:52,528 root INFO [Epoch 30, Batch=1799] Train: loss=49.3884, wer=0.3160, lr=0.0006984302957695782
15:01:21,86 root INFO [Epoch 30, Batch=1899] Train: loss=31.7025, wer=0.3028, lr=0.0006978631577988533
15:02:40,574 root INFO Start to validate epoch 30
15:03:06,919 root INFO Epoch 30, lr=0.0006973426092294633 | Train: loss=36.9414, wer=0.3009 | Val: loss=30.7243, wer=0.1912 | Time: this epoch 1906.23s, elapsed 52889.29s
15:03:06,925 root INFO Start to train epoch 31
15:04:50,374 root INFO [Epoch 31, Batch=99] Train: loss=29.9887, wer=0.2530, lr=0.0006967893911185329
15:06:18,735 root INFO [Epoch 31, Batch=199] Train: loss=33.7679, wer=0.2807, lr=0.0006962262378776445
15:07:47,162 root INFO [Epoch 31, Batch=299] Train: loss=38.4040, wer=0.2995, lr=0.000695664447874214
15:09:15,758 root INFO [Epoch 31, Batch=399] Train: loss=55.2177, wer=0.3639, lr=0.0006951040156170708
15:10:44,60 root INFO [Epoch 31, Batch=499] Train: loss=39.3217, wer=0.2782, lr=0.000694544935645961
15:12:12,431 root INFO [Epoch 31, Batch=599] Train: loss=18.2481, wer=0.3747, lr=0.0006939872025313231
15:13:40,492 root INFO [Epoch 31, Batch=699] Train: loss=36.6272, wer=0.3076, lr=0.0006934308108740672
15:15:09,1 root INFO [Epoch 31, Batch=799] Train: loss=36.9893, wer=0.2810, lr=0.0006928757553053552
15:16:38,212 root INFO [Epoch 31, Batch=899] Train: loss=33.4348, wer=0.2830, lr=0.0006923220304863833
15:18:06,548 root INFO [Epoch 31, Batch=999] Train: loss=39.8159, wer=0.3492, lr=0.0006917696311081663
15:19:35,139 root INFO [Epoch 31, Batch=1099] Train: loss=57.8043, wer=0.3496, lr=0.0006912185518913238
15:21:03,285 root INFO [Epoch 31, Batch=1199] Train: loss=35.6120, wer=0.2961, lr=0.0006906687875858682
15:22:31,998 root INFO [Epoch 31, Batch=1299] Train: loss=37.8453, wer=0.3257, lr=0.0006901203329709949
15:24:00,148 root INFO [Epoch 31, Batch=1399] Train: loss=31.2401, wer=0.2215, lr=0.0006895731828548737
15:25:29,114 root INFO [Epoch 31, Batch=1499] Train: loss=28.8431, wer=0.2360, lr=0.0006890273320744426
15:26:59,384 root INFO [Epoch 31, Batch=1599] Train: loss=37.3883, wer=0.2723, lr=0.0006884827754952036
15:28:29,698 root INFO [Epoch 31, Batch=1699] Train: loss=36.6733, wer=0.2731, lr=0.0006879395080110185
15:29:59,474 root INFO [Epoch 31, Batch=1799] Train: loss=45.3162, wer=0.3126, lr=0.0006873975245439094
15:31:27,923 root INFO [Epoch 31, Batch=1899] Train: loss=37.9657, wer=0.3277, lr=0.0006868568200438582
15:32:48,287 root INFO Start to validate epoch 31
15:33:14,727 root INFO Epoch 31, lr=0.0006863604971696708 | Train: loss=36.9483, wer=0.2988 | Val: loss=30.7692, wer=0.1880 | Time: this epoch 1807.80s, elapsed 54697.10s
15:33:14,974 root INFO [info] Save model after epoch 31

15:33:14,974 root INFO Start to train epoch 32
15:34:43,872 root INFO [Epoch 32, Batch=99] Train: loss=9.3687, wer=0.4677, lr=0.0006858329870830168
15:36:11,334 root INFO [Epoch 32, Batch=199] Train: loss=38.2635, wer=0.3055, lr=0.0006852959632851547
15:37:39,362 root INFO [Epoch 32, Batch=299] Train: loss=32.5374, wer=0.2741, lr=0.0006847601990227505
15:39:07,473 root INFO [Epoch 32, Batch=399] Train: loss=48.3919, wer=0.3274, lr=0.0006842256893799666
15:40:36,284 root INFO [Epoch 32, Batch=499] Train: loss=38.9764, wer=0.2893, lr=0.0006836924294677843
15:42:04,843 root INFO [Epoch 32, Batch=599] Train: loss=47.1787, wer=0.3218, lr=0.0006831604144238151
15:43:35,122 root INFO [Epoch 32, Batch=699] Train: loss=38.6756, wer=0.3083, lr=0.0006826296394121151
15:45:02,750 root INFO [Epoch 32, Batch=799] Train: loss=33.2397, wer=0.2922, lr=0.0006821000996230005
15:46:29,846 root INFO [Epoch 32, Batch=899] Train: loss=48.6053, wer=0.2982, lr=0.0006815717902728642
15:47:57,863 root INFO [Epoch 32, Batch=999] Train: loss=28.1720, wer=0.2533, lr=0.0006810447066039946
15:49:26,11 root INFO [Epoch 32, Batch=1099] Train: loss=35.4283, wer=0.3187, lr=0.0006805188438843952
15:50:53,868 root INFO [Epoch 32, Batch=1199] Train: loss=64.6140, wer=0.3568, lr=0.0006799941974076065
15:52:21,941 root INFO [Epoch 32, Batch=1299] Train: loss=32.0481, wer=0.2819, lr=0.0006794707624925293
15:53:50,26 root INFO [Epoch 32, Batch=1399] Train: loss=58.2146, wer=0.3285, lr=0.000678948534483249
15:55:18,434 root INFO [Epoch 32, Batch=1499] Train: loss=41.8330, wer=0.3066, lr=0.0006784275087488614
15:56:46,250 root INFO [Epoch 32, Batch=1599] Train: loss=32.3934, wer=0.2896, lr=0.0006779076806833006
15:58:14,987 root INFO [Epoch 32, Batch=1699] Train: loss=21.0551, wer=0.2848, lr=0.0006773890457051681
15:59:43,221 root INFO [Epoch 32, Batch=1799] Train: loss=39.1772, wer=0.2849, lr=0.0006768715992575627
16:01:11,999 root INFO [Epoch 32, Batch=1899] Train: loss=39.4886, wer=0.2842, lr=0.000676355336807913
16:02:31,355 root INFO Start to validate epoch 32
16:02:57,716 root INFO Epoch 32, lr=0.0006758814171851715 | Train: loss=36.3762, wer=0.2948 | Val: loss=30.3316, wer=0.1903 | Time: this epoch 1782.74s, elapsed 56480.09s
16:02:57,717 root INFO Start to train epoch 33
16:04:27,364 root INFO [Epoch 33, Batch=99] Train: loss=30.3253, wer=0.2353, lr=0.0006753776839404198
16:05:56,10 root INFO [Epoch 33, Batch=199] Train: loss=38.0683, wer=0.3149, lr=0.0006748648296756028
16:07:24,350 root INFO [Epoch 33, Batch=299] Train: loss=37.1668, wer=0.2722, lr=0.0006743531419610651
16:08:52,813 root INFO [Epoch 33, Batch=399] Train: loss=26.1849, wer=0.3357, lr=0.0006738426163810649
16:10:21,354 root INFO [Epoch 33, Batch=499] Train: loss=36.3118, wer=0.2821, lr=0.000673333248543226
16:11:50,144 root INFO [Epoch 33, Batch=599] Train: loss=31.5424, wer=0.3080, lr=0.0006728250340783786
16:13:18,644 root INFO [Epoch 33, Batch=699] Train: loss=35.4254, wer=0.2825, lr=0.0006723179686404029
16:14:46,867 root INFO [Epoch 33, Batch=799] Train: loss=43.8353, wer=0.3091, lr=0.0006718120479060715
16:16:15,357 root INFO [Epoch 33, Batch=899] Train: loss=49.7072, wer=0.3197, lr=0.0006713072675748964
16:17:43,552 root INFO [Epoch 33, Batch=999] Train: loss=32.4116, wer=0.2640, lr=0.0006708036233689736
16:19:12,327 root INFO [Epoch 33, Batch=1099] Train: loss=29.5881, wer=0.2474, lr=0.0006703011110328326
16:20:40,515 root INFO [Epoch 33, Batch=1199] Train: loss=43.6012, wer=0.3080, lr=0.0006697997263332838
16:22:07,702 root INFO [Epoch 33, Batch=1299] Train: loss=34.0607, wer=0.2772, lr=0.0006692994650592699
16:23:34,220 root INFO [Epoch 33, Batch=1399] Train: loss=30.5831, wer=0.2729, lr=0.0006688003230217168
16:25:00,368 root INFO [Epoch 33, Batch=1499] Train: loss=19.4442, wer=0.3113, lr=0.0006683022960533862
16:26:26,745 root INFO [Epoch 33, Batch=1599] Train: loss=32.2131, wer=0.2629, lr=0.0006678053800087299
16:27:52,813 root INFO [Epoch 33, Batch=1699] Train: loss=32.9781, wer=0.2482, lr=0.0006673095707637446
16:29:19,215 root INFO [Epoch 33, Batch=1799] Train: loss=40.6976, wer=0.2573, lr=0.000666814864215828
16:30:44,633 root INFO [Epoch 33, Batch=1899] Train: loss=37.9050, wer=0.3022, lr=0.0006663212562836362
16:32:01,869 root INFO Start to validate epoch 33
16:32:27,391 root INFO Epoch 33, lr=0.0006658681037927017 | Train: loss=35.5322, wer=0.2897 | Val: loss=30.5048, wer=0.1885 | Time: this epoch 1769.67s, elapsed 58249.76s
16:32:27,396 root INFO Start to train epoch 34
16:33:55,199 root INFO [Epoch 34, Batch=99] Train: loss=24.1818, wer=0.2205, lr=0.0006653864133739999
16:35:22,314 root INFO [Epoch 34, Batch=199] Train: loss=21.3976, wer=0.3127, lr=0.000664895968541847
16:36:49,786 root INFO [Epoch 34, Batch=299] Train: loss=32.5702, wer=0.2760, lr=0.0006644066066086628
16:38:17,36 root INFO [Epoch 34, Batch=399] Train: loss=39.5079, wer=0.3200, lr=0.0006639183235952481
16:39:43,843 root INFO [Epoch 34, Batch=499] Train: loss=34.1115, wer=0.2651, lr=0.0006634311155428441
16:41:10,134 root INFO [Epoch 34, Batch=599] Train: loss=34.3146, wer=0.2507, lr=0.0006629449785129979
16:42:36,788 root INFO [Epoch 34, Batch=699] Train: loss=12.3425, wer=0.3901, lr=0.0006624599085874285
16:44:03,287 root INFO [Epoch 34, Batch=799] Train: loss=45.8803, wer=0.3081, lr=0.0006619759018678947
16:45:30,116 root INFO [Epoch 34, Batch=899] Train: loss=22.8285, wer=0.2169, lr=0.0006614929544760627
16:46:56,787 root INFO [Epoch 34, Batch=999] Train: loss=44.1606, wer=0.2753, lr=0.0006610110625533763
16:48:23,203 root INFO [Epoch 34, Batch=1099] Train: loss=30.6792, wer=0.2516, lr=0.0006605302222609272
16:49:49,746 root INFO [Epoch 34, Batch=1199] Train: loss=36.4019, wer=0.2754, lr=0.0006600504297793264
16:51:16,751 root INFO [Epoch 34, Batch=1299] Train: loss=39.8766, wer=0.2759, lr=0.0006595716813085768
16:52:43,458 root INFO [Epoch 34, Batch=1399] Train: loss=67.1030, wer=0.3848, lr=0.0006590939730679472
16:54:10,512 root INFO [Epoch 34, Batch=1499] Train: loss=30.0012, wer=0.3496, lr=0.0006586173012958461
16:55:37,692 root INFO [Epoch 34, Batch=1599] Train: loss=25.8563, wer=0.2471, lr=0.0006581416622496982
16:57:04,953 root INFO [Epoch 34, Batch=1699] Train: loss=37.3366, wer=0.3211, lr=0.0006576670522058205
16:58:31,453 root INFO [Epoch 34, Batch=1799] Train: loss=33.8706, wer=0.2398, lr=0.0006571934674592998
16:59:58,21 root INFO [Epoch 34, Batch=1899] Train: loss=29.6199, wer=0.2621, lr=0.0006567209043238713
17:01:15,830 root INFO Start to validate epoch 34
17:01:41,622 root INFO Epoch 34, lr=0.0006562870453729142 | Train: loss=35.2017, wer=0.2867 | Val: loss=29.5216, wer=0.1835 | Time: this epoch 1754.23s, elapsed 60003.99s
17:01:41,845 root INFO [info] Save model after epoch 34

17:01:41,845 root INFO Start to train epoch 35
17:03:09,95 root INFO [Epoch 35, Batch=99] Train: loss=32.1301, wer=0.2772, lr=0.000655825835783953
17:04:36,276 root INFO [Epoch 35, Batch=199] Train: loss=19.4031, wer=0.2242, lr=0.0006553562146454745
17:06:02,893 root INFO [Epoch 35, Batch=299] Train: loss=37.3757, wer=0.2798, lr=0.0006548876009175901
17:07:29,852 root INFO [Epoch 35, Batch=399] Train: loss=35.4600, wer=0.2566, lr=0.0006544199910036886
17:08:56,594 root INFO [Epoch 35, Batch=499] Train: loss=28.9652, wer=0.2480, lr=0.0006539533813251088
17:10:23,83 root INFO [Epoch 35, Batch=599] Train: loss=31.5445, wer=0.2502, lr=0.0006534877683210266
17:11:49,547 root INFO [Epoch 35, Batch=699] Train: loss=40.9792, wer=0.3098, lr=0.0006530231484483392
17:13:16,115 root INFO [Epoch 35, Batch=799] Train: loss=35.5734, wer=0.2667, lr=0.0006525595181815525
17:14:43,9 root INFO [Epoch 35, Batch=899] Train: loss=43.5454, wer=0.3044, lr=0.0006520968740126688
17:16:09,319 root INFO [Epoch 35, Batch=999] Train: loss=22.4715, wer=0.3239, lr=0.0006516352124510747
17:17:36,137 root INFO [Epoch 35, Batch=1099] Train: loss=43.2137, wer=0.2721, lr=0.0006511745300234315
17:19:02,282 root INFO [Epoch 35, Batch=1199] Train: loss=33.7894, wer=0.2698, lr=0.0006507148232735646
17:20:28,975 root INFO [Epoch 35, Batch=1299] Train: loss=41.1791, wer=0.3122, lr=0.0006502560887623552
17:21:55,536 root INFO [Epoch 35, Batch=1399] Train: loss=38.0170, wer=0.3016, lr=0.0006497983230676315
17:23:22,520 root INFO [Epoch 35, Batch=1499] Train: loss=41.8986, wer=0.2830, lr=0.0006493415227840625
17:24:49,358 root INFO [Epoch 35, Batch=1599] Train: loss=39.8014, wer=0.2635, lr=0.0006488856845230502
17:26:15,959 root INFO [Epoch 35, Batch=1699] Train: loss=17.4874, wer=0.3302, lr=0.0006484308049126257
17:27:42,921 root INFO [Epoch 35, Batch=1799] Train: loss=27.3829, wer=0.3809, lr=0.0006479768805973432
17:29:09,283 root INFO [Epoch 35, Batch=1899] Train: loss=35.5616, wer=0.2601, lr=0.0006475239082381762
17:30:27,299 root INFO Start to validate epoch 35
17:30:53,80 root INFO Epoch 35, lr=0.0006471080115787439 | Train: loss=35.1612, wer=0.2849 | Val: loss=29.4010, wer=0.1827 | Time: this epoch 1751.23s, elapsed 61755.45s
17:30:53,298 root INFO [info] Save model after epoch 35

17:30:53,298 root INFO Start to train epoch 36
17:32:21,44 root INFO [Epoch 36, Batch=99] Train: loss=38.3596, wer=0.2811, lr=0.0006466658715076394
17:33:47,595 root INFO [Epoch 36, Batch=199] Train: loss=42.0605, wer=0.2880, lr=0.000646215641089272
17:35:14,550 root INFO [Epoch 36, Batch=299] Train: loss=28.2045, wer=0.3495, lr=0.0006457663497597784
17:36:41,147 root INFO [Epoch 36, Batch=399] Train: loss=34.5514, wer=0.2504, lr=0.0006453179942591119
17:38:08,346 root INFO [Epoch 36, Batch=499] Train: loss=36.9506, wer=0.3003, lr=0.0006448705713430474
17:39:34,925 root INFO [Epoch 36, Batch=599] Train: loss=24.0665, wer=0.2374, lr=0.0006444240777830838
17:41:01,704 root INFO [Epoch 36, Batch=699] Train: loss=47.6785, wer=0.2916, lr=0.0006439785103663459
17:42:28,710 root INFO [Epoch 36, Batch=799] Train: loss=24.8673, wer=0.2384, lr=0.0006435338658954864
17:43:55,745 root INFO [Epoch 36, Batch=899] Train: loss=45.9927, wer=0.2954, lr=0.000643090141188591
17:45:22,563 root INFO [Epoch 36, Batch=999] Train: loss=32.5634, wer=0.2810, lr=0.000642647333079082
17:46:49,92 root INFO [Epoch 36, Batch=1099] Train: loss=34.8181, wer=0.2655, lr=0.0006422054384156228
17:48:15,925 root INFO [Epoch 36, Batch=1199] Train: loss=36.6428, wer=0.2424, lr=0.0006417644540620249
17:49:42,740 root INFO [Epoch 36, Batch=1299] Train: loss=30.8752, wer=0.2623, lr=0.0006413243768971539
17:51:09,602 root INFO [Epoch 36, Batch=1399] Train: loss=49.0429, wer=0.3069, lr=0.0006408852038148363
17:52:36,287 root INFO [Epoch 36, Batch=1499] Train: loss=27.8425, wer=0.2927, lr=0.000640446931723768
17:54:02,954 root INFO [Epoch 36, Batch=1599] Train: loss=22.1102, wer=0.2854, lr=0.000640009557547423
17:55:29,852 root INFO [Epoch 36, Batch=1699] Train: loss=34.6693, wer=0.2916, lr=0.0006395730782239616
17:56:56,372 root INFO [Epoch 36, Batch=1799] Train: loss=36.5815, wer=0.2776, lr=0.0006391374907061421
17:58:23,321 root INFO [Epoch 36, Batch=1899] Train: loss=46.3866, wer=0.2990, lr=0.0006387027919612298
17:59:41,202 root INFO Start to validate epoch 36
18:00:07,258 root INFO Epoch 36, lr=0.0006383036514852066 | Train: loss=35.0535, wer=0.2829 | Val: loss=29.9767, wer=0.1815 | Time: this epoch 1753.96s, elapsed 63509.63s
18:00:07,630 root INFO [info] Save model after epoch 36

18:00:07,630 root INFO Start to train epoch 37
18:01:34,756 root INFO [Epoch 37, Batch=99] Train: loss=47.4074, wer=0.2933, lr=0.0006378793021167093
18:03:01,791 root INFO [Epoch 37, Batch=199] Train: loss=22.4270, wer=0.3609, lr=0.0006374471637959107
18:04:28,347 root INFO [Epoch 37, Batch=299] Train: loss=39.5801, wer=0.2818, lr=0.000637015902557319
18:05:55,199 root INFO [Epoch 37, Batch=399] Train: loss=35.8139, wer=0.2606, lr=0.000636585515438018
18:07:22,31 root INFO [Epoch 37, Batch=499] Train: loss=35.9225, wer=0.2661, lr=0.0006361559994890854
18:08:48,976 root INFO [Epoch 37, Batch=599] Train: loss=36.5875, wer=0.2974, lr=0.000635727351775508
18:10:15,847 root INFO [Epoch 37, Batch=699] Train: loss=37.2012, wer=0.3281, lr=0.0006352995693760977
18:11:42,500 root INFO [Epoch 37, Batch=799] Train: loss=15.4030, wer=0.3397, lr=0.0006348726493834074
18:13:09,474 root INFO [Epoch 37, Batch=899] Train: loss=46.8367, wer=0.3246, lr=0.0006344465889036481
18:14:36,93 root INFO [Epoch 37, Batch=999] Train: loss=35.5831, wer=0.2657, lr=0.000634021385056607
18:16:02,801 root INFO [Epoch 37, Batch=1099] Train: loss=32.3924, wer=0.2698, lr=0.0006335970349755652
18:17:29,600 root INFO [Epoch 37, Batch=1199] Train: loss=36.2637, wer=0.2873, lr=0.0006331735358072165
18:18:56,369 root INFO [Epoch 37, Batch=1299] Train: loss=33.2712, wer=0.2790, lr=0.0006327508847115873
18:20:23,182 root INFO [Epoch 37, Batch=1399] Train: loss=35.5710, wer=0.2652, lr=0.0006323290788619564
18:21:50,171 root INFO [Epoch 37, Batch=1499] Train: loss=37.9560, wer=0.2948, lr=0.0006319081154447753
18:23:17,105 root INFO [Epoch 37, Batch=1599] Train: loss=28.0668, wer=0.2590, lr=0.0006314879916595896
18:24:43,810 root INFO [Epoch 37, Batch=1699] Train: loss=39.8862, wer=0.2903, lr=0.000631068704718961
18:26:10,643 root INFO [Epoch 37, Batch=1799] Train: loss=50.5388, wer=0.3381, lr=0.000630650251848389
18:27:36,932 root INFO [Epoch 37, Batch=1899] Train: loss=27.6207, wer=0.2212, lr=0.0006302326302862345
18:28:55,202 root INFO Start to validate epoch 37
18:29:21,11 root INFO Epoch 37, lr=0.0006298491502973808 | Train: loss=34.5961, wer=0.2795 | Val: loss=30.7931, wer=0.1819 | Time: this epoch 1753.38s, elapsed 65263.38s
18:29:21,11 root INFO Start to train epoch 38
18:30:48,638 root INFO [Epoch 38, Batch=99] Train: loss=52.9197, wer=0.3031, lr=0.0006294414297380508
18:32:15,401 root INFO [Epoch 38, Batch=199] Train: loss=59.0898, wer=0.3620, lr=0.0006290262034709888
18:33:42,71 root INFO [Epoch 38, Batch=299] Train: loss=29.7438, wer=0.2550, lr=0.0006286117978634083
18:35:08,692 root INFO [Epoch 38, Batch=399] Train: loss=30.0188, wer=0.2601, lr=0.0006281982102155929
18:36:36,148 root INFO [Epoch 38, Batch=499] Train: loss=42.3965, wer=0.3088, lr=0.000627785437840244
18:38:02,660 root INFO [Epoch 38, Batch=599] Train: loss=37.0861, wer=0.3007, lr=0.0006273734780624072
18:39:29,650 root INFO [Epoch 38, Batch=699] Train: loss=23.8844, wer=0.2032, lr=0.000626962328219399
18:40:56,610 root INFO [Epoch 38, Batch=799] Train: loss=35.4521, wer=0.2785, lr=0.0006265519856607348
18:42:23,173 root INFO [Epoch 38, Batch=899] Train: loss=26.3241, wer=0.2286, lr=0.0006261424477480575
18:43:50,251 root INFO [Epoch 38, Batch=999] Train: loss=26.0697, wer=0.2783, lr=0.0006257337118550658
18:45:16,833 root INFO [Epoch 38, Batch=1099] Train: loss=32.3213, wer=0.2893, lr=0.0006253257753674438
18:46:43,502 root INFO [Epoch 38, Batch=1199] Train: loss=37.4686, wer=0.2862, lr=0.0006249186356827901
18:48:10,496 root INFO [Epoch 38, Batch=1299] Train: loss=17.9248, wer=0.2144, lr=0.000624512290210549
18:49:37,81 root INFO [Epoch 38, Batch=1399] Train: loss=42.0388, wer=0.2767, lr=0.0006241067363719408
18:51:05,210 root INFO [Epoch 38, Batch=1499] Train: loss=23.2553, wer=0.2768, lr=0.0006237019715998927
18:52:32,939 root INFO [Epoch 38, Batch=1599] Train: loss=33.2320, wer=0.3340, lr=0.0006232979933389715
18:54:00,669 root INFO [Epoch 38, Batch=1699] Train: loss=18.1153, wer=0.3582, lr=0.0006228947990453148
18:55:28,188 root INFO [Epoch 38, Batch=1799] Train: loss=30.9700, wer=0.2409, lr=0.0006224923861865645
18:56:55,178 root INFO [Epoch 38, Batch=1899] Train: loss=17.6267, wer=0.3233, lr=0.0006220907522417994
18:58:13,49 root INFO Start to validate epoch 38
18:58:39,121 root INFO Epoch 38, lr=0.000621721934791923 | Train: loss=34.2107, wer=0.2761 | Val: loss=29.7835, wer=0.1785 | Time: this epoch 1758.11s, elapsed 67021.49s
18:58:39,343 root INFO [info] Save model after epoch 38

18:58:39,343 root INFO Start to train epoch 39
19:00:06,104 root INFO [Epoch 39, Batch=99] Train: loss=31.4413, wer=0.2479, lr=0.0006213297846759326
19:01:33,120 root INFO [Epoch 39, Batch=199] Train: loss=28.8753, wer=0.2687, lr=0.0006209303954306551
19:02:59,224 root INFO [Epoch 39, Batch=299] Train: loss=34.9158, wer=0.2689, lr=0.0006205317753758135
19:04:25,771 root INFO [Epoch 39, Batch=399] Train: loss=42.6481, wer=0.3098, lr=0.0006201339220455787
19:05:52,541 root INFO [Epoch 39, Batch=499] Train: loss=31.9326, wer=0.2661, lr=0.0006197368329851748
19:07:18,779 root INFO [Epoch 39, Batch=599] Train: loss=30.2303, wer=0.2524, lr=0.0006193405057508146
19:08:45,570 root INFO [Epoch 39, Batch=699] Train: loss=12.0360, wer=0.3674, lr=0.0006189449379096365
19:10:12,50 root INFO [Epoch 39, Batch=799] Train: loss=29.5371, wer=0.3210, lr=0.0006185501270396423
19:11:38,616 root INFO [Epoch 39, Batch=899] Train: loss=39.2342, wer=0.2694, lr=0.0006181560707296344
19:13:05,113 root INFO [Epoch 39, Batch=999] Train: loss=39.6580, wer=0.3096, lr=0.0006177627665791541
19:14:31,885 root INFO [Epoch 39, Batch=1099] Train: loss=52.3416, wer=0.3118, lr=0.0006173702121984203
19:15:59,15 root INFO [Epoch 39, Batch=1199] Train: loss=23.4447, wer=0.2145, lr=0.0006169784052082686
19:17:24,992 root INFO [Epoch 39, Batch=1299] Train: loss=26.9689, wer=0.2386, lr=0.0006165873432400901
19:18:52,173 root INFO [Epoch 39, Batch=1399] Train: loss=35.9354, wer=0.2321, lr=0.0006161970239357725
19:20:18,674 root INFO [Epoch 39, Batch=1499] Train: loss=20.2445, wer=0.2292, lr=0.0006158074449476386
19:21:45,719 root INFO [Epoch 39, Batch=1599] Train: loss=28.4721, wer=0.2486, lr=0.0006154186039383889
19:23:12,575 root INFO [Epoch 39, Batch=1699] Train: loss=38.7417, wer=0.2627, lr=0.0006150304985810414
19:24:39,695 root INFO [Epoch 39, Batch=1799] Train: loss=26.3132, wer=0.2224, lr=0.0006146431265588735
19:26:08,974 root INFO [Epoch 39, Batch=1899] Train: loss=24.2833, wer=0.2186, lr=0.0006142564855653639
19:27:30,199 root INFO Start to validate epoch 39
19:27:56,615 root INFO Epoch 39, lr=0.0006139014195215962 | Train: loss=33.8977, wer=0.2735 | Val: loss=29.0350, wer=0.1754 | Time: this epoch 1757.27s, elapsed 68778.99s
19:27:56,880 root INFO [info] Save model after epoch 39

19:27:56,880 root INFO Start to train epoch 40
19:29:26,819 root INFO [Epoch 40, Batch=99] Train: loss=44.2272, wer=0.3008, lr=0.0006135238734453026
19:30:56,479 root INFO [Epoch 40, Batch=199] Train: loss=56.0019, wer=0.3565, lr=0.0006131393394849658
19:32:26,466 root INFO [Epoch 40, Batch=299] Train: loss=30.7007, wer=0.2481, lr=0.0006127555276542502
19:33:56,206 root INFO [Epoch 40, Batch=399] Train: loss=50.4607, wer=0.3028, lr=0.0006123724356957946
19:35:26,75 root INFO [Epoch 40, Batch=499] Train: loss=32.6072, wer=0.2407, lr=0.0006119900613621046
19:36:55,828 root INFO [Epoch 40, Batch=599] Train: loss=33.5181, wer=0.3350, lr=0.0006116084024154973
19:38:25,106 root INFO [Epoch 40, Batch=699] Train: loss=31.4866, wer=0.2920, lr=0.0006112274566280462
19:39:54,518 root INFO [Epoch 40, Batch=799] Train: loss=32.1035, wer=0.2606, lr=0.0006108472217815262
19:41:23,695 root INFO [Epoch 40, Batch=899] Train: loss=40.2323, wer=0.2593, lr=0.0006104676956673598
19:42:50,483 root INFO [Epoch 40, Batch=999] Train: loss=41.6771, wer=0.2812, lr=0.0006100888760865632
19:44:17,351 root INFO [Epoch 40, Batch=1099] Train: loss=32.7990, wer=0.2792, lr=0.0006097107608496925
19:45:43,962 root INFO [Epoch 40, Batch=1199] Train: loss=39.0911, wer=0.2612, lr=0.0006093333477767906
19:47:10,933 root INFO [Epoch 40, Batch=1299] Train: loss=23.4150, wer=0.3199, lr=0.0006089566346973352
19:48:37,751 root INFO [Epoch 40, Batch=1399] Train: loss=31.1969, wer=0.2296, lr=0.0006085806194501847
19:50:04,603 root INFO [Epoch 40, Batch=1499] Train: loss=40.9253, wer=0.2774, lr=0.0006082052998835276
19:51:31,69 root INFO [Epoch 40, Batch=1599] Train: loss=31.7123, wer=0.3192, lr=0.0006078306738548308
19:52:57,919 root INFO [Epoch 40, Batch=1699] Train: loss=32.0272, wer=0.3039, lr=0.0006074567392307871
19:54:25,887 root INFO [Epoch 40, Batch=1799] Train: loss=58.4978, wer=0.3333, lr=0.0006070834938872648
19:55:54,91 root INFO [Epoch 40, Batch=1899] Train: loss=21.5983, wer=0.2649, lr=0.0006067109357092573
19:57:13,951 root INFO Start to validate epoch 40
19:57:40,252 root INFO Epoch 40, lr=0.0006063687872795574 | Train: loss=33.5671, wer=0.2718 | Val: loss=29.7809, wer=0.1780 | Time: this epoch 1783.37s, elapsed 70562.62s
19:57:40,252 root INFO Start to train epoch 41
19:59:09,482 root INFO [Epoch 41, Batch=99] Train: loss=52.1267, wer=0.3497, lr=0.0006060049607769135
20:00:36,959 root INFO [Epoch 41, Batch=199] Train: loss=28.2756, wer=0.2807, lr=0.0006056343835021339
20:02:03,705 root INFO [Epoch 41, Batch=299] Train: loss=32.4716, wer=0.2802, lr=0.0006052644852305898
20:03:30,648 root INFO [Epoch 41, Batch=399] Train: loss=33.0533, wer=0.2444, lr=0.0006048952638912647
20:04:57,149 root INFO [Epoch 41, Batch=499] Train: loss=37.6818, wer=0.2719, lr=0.0006045267174219753
20:06:24,56 root INFO [Epoch 41, Batch=599] Train: loss=33.1939, wer=0.2904, lr=0.0006041588437693222
20:07:50,551 root INFO [Epoch 41, Batch=699] Train: loss=54.8352, wer=0.3558, lr=0.0006037916408886426
20:09:17,484 root INFO [Epoch 41, Batch=799] Train: loss=27.1718, wer=0.2235, lr=0.0006034251067439623
20:10:44,569 root INFO [Epoch 41, Batch=899] Train: loss=41.5324, wer=0.2680, lr=0.0006030592393079478
20:12:11,822 root INFO [Epoch 41, Batch=999] Train: loss=23.7021, wer=0.2386, lr=0.00060269403656186
20:13:39,520 root INFO [Epoch 41, Batch=1099] Train: loss=32.5928, wer=0.2787, lr=0.0006023294964955068
20:15:06,545 root INFO [Epoch 41, Batch=1199] Train: loss=37.1813, wer=0.2551, lr=0.0006019656171071971
20:16:33,952 root INFO [Epoch 41, Batch=1299] Train: loss=46.9142, wer=0.2640, lr=0.0006016023964036946
20:18:00,768 root INFO [Epoch 41, Batch=1399] Train: loss=39.9179, wer=0.3079, lr=0.0006012398324001714
20:19:28,493 root INFO [Epoch 41, Batch=1499] Train: loss=44.1202, wer=0.2816, lr=0.0006008779231201628
20:20:55,246 root INFO [Epoch 41, Batch=1599] Train: loss=45.6506, wer=0.2655, lr=0.0006005166665955221
20:22:22,558 root INFO [Epoch 41, Batch=1699] Train: loss=35.5676, wer=0.2860, lr=0.0006001560608663761
20:23:50,643 root INFO [Epoch 41, Batch=1799] Train: loss=26.1473, wer=0.2128, lr=0.0005997961039810792
20:25:17,969 root INFO [Epoch 41, Batch=1899] Train: loss=40.7699, wer=0.3001, lr=0.00059943679399617
20:26:36,413 root INFO Start to validate epoch 41
20:27:02,487 root INFO Epoch 41, lr=0.0005991067984886418 | Train: loss=33.5404, wer=0.2700 | Val: loss=29.2607, wer=0.1753 | Time: this epoch 1762.23s, elapsed 72324.86s
20:27:02,740 root INFO [info] Save model after epoch 41

20:27:02,740 root INFO Start to train epoch 42
20:28:30,253 root INFO [Epoch 42, Batch=99] Train: loss=35.5415, wer=0.2663, lr=0.0005987558803105829
20:29:57,551 root INFO [Epoch 42, Batch=199] Train: loss=27.1864, wer=0.2682, lr=0.0005983984354215175
20:31:26,828 root INFO [Epoch 42, Batch=299] Train: loss=23.2239, wer=0.2317, lr=0.0005980416299307148
20:32:55,482 root INFO [Epoch 42, Batch=399] Train: loss=31.6913, wer=0.2663, lr=0.0005976854619341833
20:34:25,80 root INFO [Epoch 42, Batch=499] Train: loss=44.6984, wer=0.3018, lr=0.0005973299295358596
20:35:53,679 root INFO [Epoch 42, Batch=599] Train: loss=27.9802, wer=0.2202, lr=0.0005969750308475662
20:37:22,625 root INFO [Epoch 42, Batch=699] Train: loss=23.3374, wer=0.3147, lr=0.0005966207639889692
20:38:51,106 root INFO [Epoch 42, Batch=799] Train: loss=13.5449, wer=0.3463, lr=0.0005962671270875362
20:40:20,159 root INFO [Epoch 42, Batch=899] Train: loss=29.0522, wer=0.2229, lr=0.0005959141182784952
20:41:49,672 root INFO [Epoch 42, Batch=999] Train: loss=33.0604, wer=0.2758, lr=0.0005955617357047932
20:43:17,987 root INFO [Epoch 42, Batch=1099] Train: loss=37.0453, wer=0.2890, lr=0.0005952099775170546
20:44:45,275 root INFO [Epoch 42, Batch=1199] Train: loss=46.5080, wer=0.3047, lr=0.0005948588418735414
20:46:12,757 root INFO [Epoch 42, Batch=1299] Train: loss=30.6945, wer=0.2556, lr=0.0005945083269401119
20:47:41,398 root INFO [Epoch 42, Batch=1399] Train: loss=38.0502, wer=0.3067, lr=0.0005941584308901809
20:49:09,863 root INFO [Epoch 42, Batch=1499] Train: loss=36.2009, wer=0.2632, lr=0.0005938091519046793
20:50:38,336 root INFO [Epoch 42, Batch=1599] Train: loss=37.8575, wer=0.2741, lr=0.0005934604881720152
20:52:07,243 root INFO [Epoch 42, Batch=1699] Train: loss=30.8457, wer=0.3208, lr=0.0005931124378880331
20:53:35,829 root INFO [Epoch 42, Batch=1799] Train: loss=30.8818, wer=0.2110, lr=0.0005927649992559762
20:55:04,335 root INFO [Epoch 42, Batch=1899] Train: loss=32.8915, wer=0.2546, lr=0.0005924181704864464
20:56:23,581 root INFO Start to validate epoch 42
20:56:50,136 root INFO Epoch 42, lr=0.0005920996251169779 | Train: loss=33.5163, wer=0.2684 | Val: loss=28.6793, wer=0.1749 | Time: this epoch 1787.40s, elapsed 74112.51s
20:56:50,703 root INFO [info] Save model after epoch 42

20:56:50,703 root INFO Start to train epoch 43
20:58:19,950 root INFO [Epoch 43, Batch=99] Train: loss=44.2864, wer=0.2532, lr=0.0005917608696189588
20:59:47,213 root INFO [Epoch 43, Batch=199] Train: loss=35.1409, wer=0.2550, lr=0.0005914157993991569
21:01:14,162 root INFO [Epoch 43, Batch=299] Train: loss=30.0396, wer=0.2277, lr=0.000591071332132821
21:02:41,313 root INFO [Epoch 43, Batch=399] Train: loss=25.9008, wer=0.2123, lr=0.0005907274660660574
21:04:07,826 root INFO [Epoch 43, Batch=499] Train: loss=44.6336, wer=0.2882, lr=0.0005903841994521066
21:05:34,769 root INFO [Epoch 43, Batch=599] Train: loss=30.6662, wer=0.2743, lr=0.000590041530551306
21:07:01,683 root INFO [Epoch 43, Batch=699] Train: loss=50.4239, wer=0.3056, lr=0.0005896994576310531
21:08:28,403 root INFO [Epoch 43, Batch=799] Train: loss=25.6296, wer=0.2184, lr=0.0005893579789657681
21:09:55,480 root INFO [Epoch 43, Batch=899] Train: loss=27.3677, wer=0.2233, lr=0.0005890170928368582
21:11:22,237 root INFO [Epoch 43, Batch=999] Train: loss=30.3621, wer=0.2963, lr=0.0005886767975326806
21:12:49,280 root INFO [Epoch 43, Batch=1099] Train: loss=39.4409, wer=0.2705, lr=0.0005883370913485065
21:14:16,308 root INFO [Epoch 43, Batch=1199] Train: loss=38.5778, wer=0.2502, lr=0.0005879979725864858
21:15:43,298 root INFO [Epoch 43, Batch=1299] Train: loss=17.6889, wer=0.3575, lr=0.0005876594395556106
